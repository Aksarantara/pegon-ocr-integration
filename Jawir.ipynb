{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-jxbsVmqj2ca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU \t Memory\t Free\t         RAM\n",
      "0:\t 14.03%\t 34851 MiB\t 0 %\n",
      "1:\t  0.01%\t 40534 MiB\t 0 %\n",
      "2:\t  2.73%\t 39429 MiB\t 0 %\n",
      "3:\t  0.01%\t 40534 MiB\t 0 %\n",
      "4:\t  0.01%\t 40534 MiB\t 0 %\n",
      "5:\t  0.01%\t 40534 MiB\t 0 %\n",
      "6:\t  0.01%\t 40534 MiB\t 0 %\n",
      "7:\t 91.09%\t 3611 MiB\t 57 %\n"
     ]
    }
   ],
   "source": [
    "# Melihat GPU yang tersedia dan penggunaannya.\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total,memory.free,utilization.gpu --format=csv,noheader | awk -F \",\" 'BEGIN{printf \"%-3s \\t%7s\\t%5s\\t%12s\\n\", \"GPU\", \"Memory\", \"Free\", \"RAM\"}{printf \"%s:\\t%6.2f%%\\t%7s\\t%s\\n\", $1, ($2/$3)*100, $4, $5}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jfVIZzEKHZB"
   },
   "source": [
    "#Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA6LfjkeKLdw"
   },
   "source": [
    "###Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7z5hP1qiKKuX"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Simple RNN based encoder network\n",
    "    '''\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim ,\n",
    "                       rnn_type = 'gru', layers = 1,\n",
    "                       bidirectional =False,\n",
    "                       dropout = 0, device = \"cpu\"):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim #src_vocab_sz\n",
    "        self.enc_embed_dim = embed_dim\n",
    "        self.enc_hidden_dim = hidden_dim\n",
    "        self.enc_rnn_type = rnn_type\n",
    "        self.enc_layers = layers\n",
    "        self.enc_directions = 2 if bidirectional else 1\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_dim, self.enc_embed_dim)\n",
    "\n",
    "        if self.enc_rnn_type == \"gru\":\n",
    "            self.enc_rnn = nn.GRU(input_size= self.enc_embed_dim,\n",
    "                          hidden_size= self.enc_hidden_dim,\n",
    "                          num_layers= self.enc_layers,\n",
    "                          bidirectional= bidirectional)\n",
    "        elif self.enc_rnn_type == \"lstm\":\n",
    "            self.enc_rnn = nn.LSTM(input_size= self.enc_embed_dim,\n",
    "                          hidden_size= self.enc_hidden_dim,\n",
    "                          num_layers= self.enc_layers,\n",
    "                          bidirectional= bidirectional)\n",
    "        else:\n",
    "            raise Exception(\"unknown RNN type mentioned\")\n",
    "\n",
    "    def forward(self, x, x_sz, hidden = None):\n",
    "        '''\n",
    "        x_sz: (batch_size, 1) -  Unpadded sequence lengths used for pack_pad\n",
    "\n",
    "        Return:\n",
    "            output: (batch_size, max_length, hidden_dim)\n",
    "            hidden: (n_layer*num_directions, batch_size, hidden_dim) | if LSTM tuple -(h_n, c_n)\n",
    "\n",
    "        '''\n",
    "        batch_sz = x.shape[0]\n",
    "        # x: batch_size, max_length, enc_embed_dim\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        ## pack the padded data\n",
    "        # x: max_length, batch_size, enc_embed_dim -> for pack_pad\n",
    "        x = x.permute(1,0,2)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, x_sz, enforce_sorted=False) # unpad\n",
    "\n",
    "        # output: packed_size, batch_size, enc_embed_dim --> hidden from all timesteps\n",
    "        # hidden: n_layer**num_directions, batch_size, hidden_dim | if LSTM (h_n, c_n)\n",
    "        output, hidden = self.enc_rnn(x)\n",
    "\n",
    "        ## pad the sequence to the max length in the batch\n",
    "        # output: max_length, batch_size, enc_emb_dim*directions)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "\n",
    "        # output: batch_size, max_length, hidden_dim\n",
    "        output = output.permute(1,0,2)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1ZMk_d7KpUf"
   },
   "source": [
    "###Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "o6toJQq-Kspw"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Used as decoder stage\n",
    "    '''\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim,\n",
    "                       rnn_type = 'gru', layers = 1,\n",
    "                       use_attention = True,\n",
    "                       enc_outstate_dim = None, # enc_directions * enc_hidden_dim\n",
    "                       dropout = 0, device = \"cpu\"):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim #tgt_vocab_sz\n",
    "        self.dec_hidden_dim = hidden_dim\n",
    "        self.dec_embed_dim = embed_dim\n",
    "        self.dec_rnn_type = rnn_type\n",
    "        self.dec_layers = layers\n",
    "        self.use_attention = use_attention\n",
    "        self.device = device\n",
    "        if self.use_attention:\n",
    "            self.enc_outstate_dim = enc_outstate_dim if enc_outstate_dim else hidden_dim\n",
    "        else:\n",
    "            self.enc_outstate_dim = 0\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.dec_embed_dim)\n",
    "\n",
    "        if self.dec_rnn_type == 'gru':\n",
    "            self.dec_rnn = nn.GRU(input_size= self.dec_embed_dim + self.enc_outstate_dim, # to concat attention_output\n",
    "                          hidden_size= self.dec_hidden_dim, # previous Hidden\n",
    "                          num_layers= self.dec_layers,\n",
    "                          batch_first = True )\n",
    "        elif self.dec_rnn_type == \"lstm\":\n",
    "            self.dec_rnn = nn.LSTM(input_size= self.dec_embed_dim + self.enc_outstate_dim, # to concat attention_output\n",
    "                          hidden_size= self.dec_hidden_dim, # previous Hidden\n",
    "                          num_layers= self.dec_layers,\n",
    "                          batch_first = True )\n",
    "        else:\n",
    "            raise Exception(\"unknown RNN type mentioned\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.dec_hidden_dim, self.dec_embed_dim), nn.LeakyReLU(),\n",
    "            # nn.Linear(self.dec_embed_dim, self.dec_embed_dim), nn.LeakyReLU(), # removing to reduce size\n",
    "            nn.Linear(self.dec_embed_dim, self.output_dim),\n",
    "            )\n",
    "\n",
    "        ##----- Attention ----------\n",
    "        if self.use_attention:\n",
    "            self.W1 = nn.Linear( self.enc_outstate_dim, self.dec_hidden_dim)\n",
    "            self.W2 = nn.Linear( self.dec_hidden_dim, self.dec_hidden_dim)\n",
    "            self.V = nn.Linear( self.dec_hidden_dim, 1)\n",
    "\n",
    "    def attention(self, x, hidden, enc_output):\n",
    "        '''\n",
    "        x: (batch_size, 1, dec_embed_dim) -> after Embedding\n",
    "        enc_output: batch_size, max_length, enc_hidden_dim *num_directions\n",
    "        hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
    "        '''\n",
    "\n",
    "        ## perform addition to calculate the score\n",
    "\n",
    "        # hidden_with_time_axis: batch_size, 1, hidden_dim\n",
    "        ## hidden_with_time_axis = hidden.permute(1, 0, 2) ## replaced with below 2lines\n",
    "        hidden_with_time_axis = torch.sum(hidden, axis=0)\n",
    "\n",
    "        hidden_with_time_axis = hidden_with_time_axis.unsqueeze(1)\n",
    "\n",
    "        # score: batch_size, max_length, hidden_dim\n",
    "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights: batch_size, max_length, 1\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_dim)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        # context_vector: batch_size, 1, hidden_dim\n",
    "        context_vector = context_vector.unsqueeze(1)\n",
    "\n",
    "        # attend_out (batch_size, 1, dec_embed_dim + hidden_size)\n",
    "        attend_out = torch.cat((context_vector, x), -1)\n",
    "\n",
    "        return attend_out, attention_weights\n",
    "\n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        '''\n",
    "        x: (batch_size, 1)\n",
    "        enc_output: batch_size, max_length, dec_embed_dim\n",
    "        hidden: n_layer, batch_size, hidden_size | lstm: (h_n, c_n)\n",
    "        '''\n",
    "        if (hidden is None) and (self.use_attention is False):\n",
    "            raise Exception( \"No use of a decoder with No attention and No Hidden\")\n",
    "\n",
    "        batch_sz = x.shape[0]\n",
    "\n",
    "        if hidden is None:\n",
    "            # hidden: n_layers, batch_size, hidden_dim\n",
    "            hid_for_att = torch.zeros((self.dec_layers, batch_sz,\n",
    "                                    self.dec_hidden_dim )).to(self.device)\n",
    "        elif self.dec_rnn_type == 'lstm':\n",
    "            hid_for_att = hidden[0] # h_n\n",
    "        else:\n",
    "            hid_for_att = hidden\n",
    "\n",
    "        # x (batch_size, 1, dec_embed_dim) -> after embedding\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if self.use_attention:\n",
    "            # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n",
    "            # aw: (batch_size, max_length, 1)\n",
    "            x, aw = self.attention( x, hid_for_att, enc_output)\n",
    "        else:\n",
    "            x, aw = x, 0\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, n_layers, hidden_size)\n",
    "        # hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
    "        output, hidden = self.dec_rnn(x, hidden) if hidden is not None else self.dec_rnn(x)\n",
    "\n",
    "        # output :shp: (batch_size * 1, hidden_size)\n",
    "        output =  output.view(-1, output.size(2))\n",
    "\n",
    "        # output :shp: (batch_size * 1, output_dim)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden, aw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOlVUXqrK1eb"
   },
   "source": [
    "### Seq2Seq Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vPaXxUy3LGz3"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    Used to construct seq2seq architecture with encoder decoder objects\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, pass_enc2dec_hid=False, dropout = 0, device = \"cpu\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.pass_enc2dec_hid = pass_enc2dec_hid\n",
    "\n",
    "        if self.pass_enc2dec_hid:\n",
    "            assert decoder.dec_hidden_dim == encoder.enc_hidden_dim, \"Hidden Dimension of encoder and decoder must be same, or unset `pass_enc2dec_hid`\"\n",
    "        if decoder.use_attention:\n",
    "            assert decoder.enc_outstate_dim == encoder.enc_directions*encoder.enc_hidden_dim,\"Set `enc_out_dim` correctly in decoder\"\n",
    "        assert self.pass_enc2dec_hid or decoder.use_attention, \"No use of a decoder with No attention and No Hidden from Encoder\"\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_sz, teacher_forcing_ratio = 0):\n",
    "        '''\n",
    "        src: (batch_size, sequence_len.padded)\n",
    "        tgt: (batch_size, sequence_len.padded)\n",
    "        src_sz: [batch_size, 1] -  Unpadded sequence lengths\n",
    "        '''\n",
    "        batch_size = tgt.shape[0]\n",
    "\n",
    "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
    "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
    "        enc_output, enc_hidden = self.encoder(src, src_sz)\n",
    "\n",
    "        if self.pass_enc2dec_hid:\n",
    "           # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            dec_hidden = enc_hidden\n",
    "        else:\n",
    "            # dec_hidden -> Will be initialized to zeros internally\n",
    "            dec_hidden = None\n",
    "\n",
    "        # pred_vecs: (batch_size, output_dim, sequence_sz) -> shape required for CELoss\n",
    "        pred_vecs = torch.zeros(batch_size, self.decoder.output_dim, tgt.size(1)).to(self.device)\n",
    "\n",
    "        # dec_input: (batch_size, 1)\n",
    "        dec_input = tgt[:,0].unsqueeze(1) # initialize to start token\n",
    "        pred_vecs[:,1,0] = 1 # Initialize to start tokens all batches\n",
    "        for t in range(1, tgt.size(1)):\n",
    "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            # dec_output: batch_size, output_dim\n",
    "            # dec_input: (batch_size, 1)\n",
    "            dec_output, dec_hidden, _ = self.decoder( dec_input,\n",
    "                                               dec_hidden,\n",
    "                                               enc_output,  )\n",
    "            pred_vecs[:,:,t] = dec_output\n",
    "\n",
    "            # # prediction: batch_size\n",
    "            prediction = torch.argmax(dec_output, dim=1)\n",
    "\n",
    "            # Teacher Forcing\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                dec_input = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                dec_input = prediction.unsqueeze(1)\n",
    "\n",
    "        return pred_vecs #(batch_size, output_dim, sequence_sz)\n",
    "\n",
    "    def inference(self, src, max_tgt_sz=50, debug = 0):\n",
    "        '''\n",
    "        single input only, No batch Inferencing\n",
    "        src: (sequence_len)\n",
    "        debug: if True will return attention weights also\n",
    "        '''\n",
    "        batch_size = 1\n",
    "        start_tok = src[0]\n",
    "        end_tok = src[-1]\n",
    "        src_sz = torch.tensor([len(src)])\n",
    "        src_ = src.unsqueeze(0)\n",
    "\n",
    "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
    "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
    "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
    "\n",
    "        if self.pass_enc2dec_hid:\n",
    "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            dec_hidden = enc_hidden\n",
    "        else:\n",
    "            # dec_hidden -> Will be initialized to zeros internally\n",
    "            dec_hidden = None\n",
    "\n",
    "        # pred_arr: (sequence_sz, 1) -> shape required for CELoss\n",
    "        pred_arr = torch.zeros(max_tgt_sz, 1).to(self.device)\n",
    "        if debug: attend_weight_arr = torch.zeros(max_tgt_sz, len(src)).to(self.device)\n",
    "\n",
    "        # dec_input: (batch_size, 1)\n",
    "        dec_input = start_tok.view(1,1) # initialize to start token\n",
    "        pred_arr[0] = start_tok.view(1,1) # initialize to start token\n",
    "        for t in range(max_tgt_sz):\n",
    "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            # dec_output: batch_size, output_dim\n",
    "            # dec_input: (batch_size, 1)\n",
    "            dec_output, dec_hidden, aw = self.decoder( dec_input,\n",
    "                                               dec_hidden,\n",
    "                                               enc_output,  )\n",
    "            # prediction :shp: (1,1)\n",
    "            prediction = torch.argmax(dec_output, dim=1)\n",
    "            dec_input = prediction.unsqueeze(1)\n",
    "            pred_arr[t] = prediction\n",
    "            if debug: attend_weight_arr[t] = aw.squeeze(-1)\n",
    "\n",
    "            if torch.eq(prediction, end_tok):\n",
    "                break\n",
    "\n",
    "        if debug: return pred_arr.squeeze(), attend_weight_arr\n",
    "        # pred_arr :shp: (sequence_len)\n",
    "        return pred_arr.squeeze().to(dtype=torch.long)\n",
    "\n",
    "\n",
    "    def active_beam_inference(self, src, beam_width=3, max_tgt_sz=50):\n",
    "        ''' Active beam Search based decoding\n",
    "        src: (sequence_len)\n",
    "        '''\n",
    "        def _avg_score(p_tup):\n",
    "            ''' Used for Sorting\n",
    "            TODO: Dividing by length of sequence power alpha as hyperparam\n",
    "            '''\n",
    "            return p_tup[0]\n",
    "\n",
    "        batch_size = 1\n",
    "        start_tok = src[0]\n",
    "        end_tok = src[-1]\n",
    "        src_sz = torch.tensor([len(src)])\n",
    "        src_ = src.unsqueeze(0)\n",
    "\n",
    "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
    "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
    "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
    "\n",
    "        if self.pass_enc2dec_hid:\n",
    "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            init_dec_hidden = enc_hidden\n",
    "        else:\n",
    "            # dec_hidden -> Will be initialized to zeros internally\n",
    "            init_dec_hidden = None\n",
    "\n",
    "        # top_pred[][0] = Σ-log_softmax\n",
    "        # top_pred[][1] = sequence torch.tensor shape: (1)\n",
    "        # top_pred[][2] = dec_hidden\n",
    "        top_pred_list = [ (0, start_tok.unsqueeze(0) , init_dec_hidden) ]\n",
    "\n",
    "        for t in range(max_tgt_sz):\n",
    "            cur_pred_list = []\n",
    "\n",
    "            for p_tup in top_pred_list:\n",
    "                if p_tup[1][-1] == end_tok:\n",
    "                    cur_pred_list.append(p_tup)\n",
    "                    continue\n",
    "\n",
    "                # dec_hidden: dec_layers, 1, hidden_dim\n",
    "                # dec_output: 1, output_dim\n",
    "                dec_output, dec_hidden, _ = self.decoder( x = p_tup[1][-1].view(1,1), #dec_input: (1,1)\n",
    "                                                    hidden = p_tup[2],\n",
    "                                                    enc_output = enc_output, )\n",
    "\n",
    "                ## π{prob} = Σ{log(prob)} -> to prevent diminishing\n",
    "                # dec_output: (1, output_dim)\n",
    "                dec_output = nn.functional.log_softmax(dec_output, dim=1)\n",
    "                # pred_topk.values & pred_topk.indices: (1, beam_width)\n",
    "                pred_topk = torch.topk(dec_output, k=beam_width, dim=1)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    sig_logsmx_ = p_tup[0] + pred_topk.values[0][i]\n",
    "                    # seq_tensor_ : (seq_len)\n",
    "                    seq_tensor_ = torch.cat( (p_tup[1], pred_topk.indices[0][i].view(1)) )\n",
    "\n",
    "                    cur_pred_list.append( (sig_logsmx_, seq_tensor_, dec_hidden) )\n",
    "\n",
    "            cur_pred_list.sort(key = _avg_score, reverse =True) # Maximized order\n",
    "            top_pred_list = cur_pred_list[:beam_width]\n",
    "\n",
    "            # check if end_tok of all topk\n",
    "            end_flags_ = [1 if t[1][-1] == end_tok else 0 for t in top_pred_list]\n",
    "            if beam_width == sum( end_flags_ ): break\n",
    "\n",
    "        pred_tnsr_list = [t[1] for t in top_pred_list ]\n",
    "\n",
    "        return pred_tnsr_list\n",
    "\n",
    "    def passive_beam_inference(self, src, beam_width = 7, max_tgt_sz=50):\n",
    "        '''\n",
    "        Passive Beam search based inference\n",
    "        src: (sequence_len)\n",
    "        '''\n",
    "        def _avg_score(p_tup):\n",
    "            ''' Used for Sorting\n",
    "            TODO: Dividing by length of sequence power alpha as hyperparam\n",
    "            '''\n",
    "            return  p_tup[0]\n",
    "\n",
    "        def _beam_search_topk(topk_obj, start_tok, beam_width):\n",
    "            ''' search for sequence with maxim prob\n",
    "            topk_obj[x]: .values & .indices shape:(1, beam_width)\n",
    "            '''\n",
    "            # top_pred_list[x]: tuple(prob, seq_tensor)\n",
    "            top_pred_list = [ (0, start_tok.unsqueeze(0) ), ]\n",
    "\n",
    "            for obj in topk_obj:\n",
    "                new_lst_ = list()\n",
    "                for itm in top_pred_list:\n",
    "                    for i in range(beam_width):\n",
    "                        sig_logsmx_ = itm[0] + obj.values[0][i]\n",
    "                        seq_tensor_ = torch.cat( (itm[1] , obj.indices[0][i].view(1) ) )\n",
    "                        new_lst_.append( (sig_logsmx_, seq_tensor_) )\n",
    "\n",
    "                new_lst_.sort(key = _avg_score, reverse =True)\n",
    "                top_pred_list = new_lst_[:beam_width]\n",
    "            return top_pred_list\n",
    "\n",
    "        batch_size = 1\n",
    "        start_tok = src[0]\n",
    "        end_tok = src[-1]\n",
    "        src_sz = torch.tensor([len(src)])\n",
    "        src_ = src.unsqueeze(0)\n",
    "\n",
    "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
    "\n",
    "        if self.pass_enc2dec_hid:\n",
    "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
    "            dec_hidden = enc_hidden\n",
    "        else:\n",
    "            # dec_hidden -> Will be initialized to zeros internally\n",
    "            dec_hidden = None\n",
    "\n",
    "        # dec_input: (1, 1)\n",
    "        dec_input = start_tok.view(1,1) # initialize to start token\n",
    "\n",
    "\n",
    "        topk_obj = []\n",
    "        for t in range(max_tgt_sz):\n",
    "            dec_output, dec_hidden, aw = self.decoder( dec_input,\n",
    "                                               dec_hidden,\n",
    "                                               enc_output,  )\n",
    "\n",
    "            ## π{prob} = Σ{log(prob)} -> to prevent diminishing\n",
    "            # dec_output: (1, output_dim)\n",
    "            dec_output = nn.functional.log_softmax(dec_output, dim=1)\n",
    "            # pred_topk.values & pred_topk.indices: (1, beam_width)\n",
    "            pred_topk = torch.topk(dec_output, k=beam_width, dim=1)\n",
    "\n",
    "            topk_obj.append(pred_topk)\n",
    "\n",
    "            # dec_input: (1, 1)\n",
    "            dec_input = pred_topk.indices[0][0].view(1,1)\n",
    "            if torch.eq(dec_input, end_tok):\n",
    "                break\n",
    "\n",
    "        top_pred_list = _beam_search_topk(topk_obj, start_tok, beam_width)\n",
    "        pred_tnsr_list = [t[1] for t in top_pred_list ]\n",
    "\n",
    "        return pred_tnsr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjNms0lEji1_"
   },
   "source": [
    "#Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdy2XH9fldK8"
   },
   "source": [
    "### Unicodes\n",
    "\n",
    "Add necessary Unicodes for specific script(langauge) below as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LC0tg-klP9D"
   },
   "source": [
    "### Glyph handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "MWXKBNkWjMhk"
   },
   "outputs": [],
   "source": [
    "class GlyphStrawboss():\n",
    "    def __init__(self, lang_script):\n",
    "        \"\"\" list of letters in a language in unicode\n",
    "        lang: List with unicodes\n",
    "        \"\"\"\n",
    "        self.glyphs = lang_script\n",
    "\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = {}\n",
    "        self._create_index()\n",
    "\n",
    "    def _create_index(self):\n",
    "\n",
    "        self.char2idx['_'] = 0  #pad\n",
    "        self.char2idx['$'] = 1  #start\n",
    "        self.char2idx['#'] = 2  #end\n",
    "        self.char2idx['*'] = 3  #Mask\n",
    "        self.char2idx[\"'\"] = 4  #apostrophe U+0027\n",
    "        self.char2idx['%'] = 5  #unused\n",
    "        self.char2idx['!'] = 6  #unused\n",
    "        \n",
    "        self.glyphs = [char for char in self.glyphs if char not in self.char2idx]\n",
    "\n",
    "        # letter to index mapping\n",
    "        for idx, char in enumerate(self.glyphs):\n",
    "            self.char2idx[char] = idx + 7 # +7 token initially\n",
    "\n",
    "        # index to letter mapping\n",
    "        for char, idx in self.char2idx.items():\n",
    "            self.idx2char[idx] = char\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "\n",
    "    def word2xlitvec(self, word):\n",
    "        \"\"\" Converts given string of gyphs(word) to vector(numpy)\n",
    "        Also adds tokens for start and end\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec = [self.char2idx['$']] #start token\n",
    "            for i in list(word):\n",
    "                vec.append(self.char2idx[i])\n",
    "            vec.append(self.char2idx['#']) #end token\n",
    "\n",
    "            vec = np.asarray(vec, dtype=np.int64)\n",
    "            return vec\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"Error In word:\", word, \"Error Char not in Token:\", error)\n",
    "            sys.exit()\n",
    "\n",
    "    def xlitvec2word(self, vector):\n",
    "        \"\"\" Converts vector(numpy) to string of glyphs(word)\n",
    "        \"\"\"\n",
    "        char_list = []\n",
    "        for i in vector:\n",
    "            char_list.append(self.idx2char[i])\n",
    "\n",
    "        word = \"\".join(char_list).replace('$','').replace('#','') # remove tokens\n",
    "        word = word.replace(\"_\", \"\").replace('*','') # remove tokens\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5yexIStOzmq"
   },
   "source": [
    "Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "eF5vDeCnWFvO"
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"dataset.json\"\n",
    "\n",
    "with open(TRAIN_FILE, 'r') as f:\n",
    "    old_json = json.load(f)\n",
    "new_json = {}\n",
    "\n",
    "for elem in old_json:\n",
    "    new_json[elem[0].lower()] = [elem[1]]\n",
    "\n",
    "TRAIN_FILE = \"train_dataset.json\"\n",
    "TEST_FILE = \"test_dataset.json\"\n",
    "\n",
    "with open(TRAIN_FILE, \"w\") as outfile:\n",
    "    json.dump(new_json, outfile)\n",
    "\n",
    "with open(TEST_FILE, \"w\") as outfile:\n",
    "    json.dump(dict(list(new_json.items())[:int(len(new_json)/1000)]), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "YJ2Zy3W7diVq"
   },
   "outputs": [],
   "source": [
    "indoarab_num = [chr(alpha) for alpha in range(48, 58)]\n",
    "\n",
    "english_lower_script = set()\n",
    "\n",
    "jawi_script = jawi_script = {\n",
    "    chr(0x200c), # ZeroWidth-NonJoiner U+200c\n",
    "    chr(0x200d), # ZeroWidthJoiner U+200d\n",
    "}\n",
    "\n",
    "for elem in old_json:\n",
    "  for char in elem[0]:\n",
    "    english_lower_script.add(char.lower())\n",
    "\n",
    "  for char in elem[1]:\n",
    "    jawi_script.add(char)\n",
    "\n",
    "english_lower_script, jawi_script = list(english_lower_script), list(jawi_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "noEImVcnO5J8"
   },
   "outputs": [],
   "source": [
    "# from `Datahandling` section\n",
    "# with open('jawi_glyph.pkl', \"rb\") as file:\n",
    "#     src_glyph = pickle.load(file)\n",
    "# with open('rumi_glyph.pkl', \"rb\") as file:\n",
    "#     tgt_glyph = pickle.load(file)\n",
    "\n",
    "src_glyph = GlyphStrawboss(jawi_script)\n",
    "tgt_glyph = GlyphStrawboss(english_lower_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XLm3rKEmVma"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "9g52_kqlmd_5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class XlitData(Dataset):\n",
    "    \"\"\" Backtransliteration from English to Native Language\n",
    "    JSON format only\n",
    "    depends on: Numpy\n",
    "    \"\"\"\n",
    "    def __init__(self, src_glyph_obj, tgt_glyph_obj,\n",
    "                    json_file, file_map = \"LangEn\",\n",
    "                    padding = True, max_seq_size = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        padding: Set True if Padding with zeros is required for Batching\n",
    "        max_seq_size: Size for Padding both input and output, Longer words will be truncated\n",
    "                      If unset computes maximum of source, target seperate\n",
    "        \"\"\"\n",
    "        #Load data\n",
    "        if file_map == \"LangEn\": # output-input\n",
    "            tgt_str, src_str = self._json2_k_v(json_file)\n",
    "        elif file_map == \"EnLang\": # input-output\n",
    "            src_str, tgt_str = self._json2_k_v(json_file)\n",
    "        else:\n",
    "            raise Exception('Unknown JSON structure')\n",
    "\n",
    "        self.src_glyph = src_glyph_obj\n",
    "        self.tgt_glyph = tgt_glyph_obj\n",
    "\n",
    "        __svec = self.src_glyph.word2xlitvec\n",
    "        __tvec = self.tgt_glyph.word2xlitvec\n",
    "        self.src = [ __svec(s)  for s in src_str]\n",
    "        self.tgt = [ __tvec(s)  for s in tgt_str]\n",
    "\n",
    "        self.tgt_class_weights = self._char_class_weights(self.tgt)\n",
    "\n",
    "        self.padding = padding\n",
    "        if max_seq_size:\n",
    "            self.max_tgt_size = max_seq_size\n",
    "            self.max_src_size = max_seq_size\n",
    "        else:\n",
    "            self.max_src_size = max(len(t) for t in self.src)\n",
    "            self.max_tgt_size = max(len(t) for t in self.tgt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_sz = len(self.src[index])\n",
    "        y_sz = len(self.tgt[index])\n",
    "        if self.padding:\n",
    "            x = self._pad_sequence(self.src[index], self.max_src_size)\n",
    "            y = self._pad_sequence(self.tgt[index], self.max_tgt_size)\n",
    "        else:\n",
    "            x = self.src[index]\n",
    "            y = self.tgt[index]\n",
    "        return x,y, x_sz\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "\n",
    "    def _json2_k_v(self, json_file):\n",
    "        ''' Convert JSON lang pairs to Key-Value lists with indexwise one2one correspondance\n",
    "        '''\n",
    "        with open(json_file, 'r', encoding = \"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        x = []; y = []\n",
    "        for k in data:\n",
    "            for v in data[k]:\n",
    "                x.append(k); y.append(v)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def _pad_sequence(self, x, max_len):\n",
    "        \"\"\" Pad sequence to maximum length;\n",
    "        Pads zero if word < max\n",
    "        Clip word if word > max\n",
    "        \"\"\"\n",
    "        padded = np.zeros((max_len), dtype=np.int64)\n",
    "        if len(x) > max_len: padded[:] = x[:max_len]\n",
    "        else: padded[:len(x)] = x\n",
    "        return padded\n",
    "\n",
    "    def _char_class_weights(self, x_list, scale = 10):\n",
    "        \"\"\"For handling class imbalance in the characters\n",
    "        Return: 1D-tensor will be fed to CEloss weights for error calculation\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        full_list = []\n",
    "        for x in x_list:\n",
    "            full_list += list(x)\n",
    "        count_dict = dict(Counter(full_list))\n",
    "\n",
    "        class_weights = np.ones(self.tgt_glyph.size(), dtype = np.float32)\n",
    "        \n",
    "        for k in count_dict:\n",
    "            class_weights[k] = (1/count_dict[k]) * scale\n",
    "\n",
    "        return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pTRIOhkJ1pY"
   },
   "source": [
    "### Merge JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "f-OncNgRkPNl"
   },
   "outputs": [],
   "source": [
    "def merge_xlit_jsons(filepath_list, save_prefix = \"\"):\n",
    "    \"\"\"\n",
    "    Merge JSON files into single file wrt keys\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for fpath in filepath_list:\n",
    "        with open(fpath, 'r', encoding = \"utf-8\") as f:\n",
    "            data_list.append(json.load(f))\n",
    "\n",
    "    whole_dict = dict()\n",
    "    for dat in data_list:\n",
    "        for dk in dat:\n",
    "            whole_dict[dk] = set()\n",
    "\n",
    "    for dat in data_list:\n",
    "        for dk in dat:\n",
    "            whole_dict[dk].update(dat[dk])\n",
    "\n",
    "    for k in whole_dict:\n",
    "        whole_dict[k] = list(whole_dict[k])\n",
    "\n",
    "    print(\"Total Key count:\", len(whole_dict))\n",
    "    save_path = save_prefix+\"merged_file.json\"\n",
    "    with open(save_path,\"w\", encoding = \"utf-8\") as f:\n",
    "        json.dump(whole_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k81RNgwckP3w"
   },
   "source": [
    "#Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "-CFrFoBwnbxu"
   },
   "outputs": [],
   "source": [
    "def LOG2CSV(data, csv_file, flag = 'a'):\n",
    "    '''\n",
    "    data: List of elements to be written\n",
    "    '''\n",
    "    with open(csv_file, flag) as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow(data)\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lFtPQyzJe2U"
   },
   "source": [
    "### Weights related utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "kK0edtDzIkTx"
   },
   "outputs": [],
   "source": [
    "def load_pretrained(model, weight_path, device, flexible = False):\n",
    "    if not weight_path:\n",
    "        return model\n",
    "\n",
    "    pretrain_dict = torch.load(weight_path) if device == 'cuda' else torch.load(weight_path, map_location=torch.device('cpu'))\n",
    "    model_dict = model.state_dict()\n",
    "    if flexible:\n",
    "        pretrain_dict = {k: v for k, v in pretrain_dict.items() if k in model_dict}\n",
    "    print(\"Pretrained layers:\", pretrain_dict.keys())\n",
    "    model_dict.update(pretrain_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "def count_train_param(model):\n",
    "    train_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('The model has {} trainable parameters'.format(train_params_count))\n",
    "    return train_params_count\n",
    "\n",
    "def freeze_params(model, exclusion_list = []):\n",
    "    ## TODO: Exclusion lists\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaNeFZknJlzL"
   },
   "source": [
    "### Accuracy Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "lr_0a0MfJlgA"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(pred_tnsr, tgt_tnsr, glyph_obj):\n",
    "    '''Simple accuracy calculation for char2char seq TRAINING phase\n",
    "    pred_tnsr: torch tensor :shp: (batch, voc_size, seq_len)\n",
    "    tgt_tnsr: torch tensor :shp: (batch, seq_len)\n",
    "    '''\n",
    "    pred_seq = torch.argmax(pred_tnsr, dim=1)\n",
    "    batch_sz = pred_seq.shape[0]\n",
    "    crt_cnt = 0\n",
    "    for i in range(batch_sz):\n",
    "        pred = glyph_obj.xlitvec2word(pred_seq[i,:].cpu().numpy())\n",
    "        tgt = glyph_obj.xlitvec2word(tgt_tnsr[i,:].cpu().numpy())\n",
    "        if pred == tgt:\n",
    "            crt_cnt += 1\n",
    "    return torch.tensor(crt_cnt/batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOHOVbdoJ-ey"
   },
   "source": [
    "#Training Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6cTnqVTNRJl"
   },
   "source": [
    "##Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "mImpycFFFujt"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "INST_NAME = \"Training_2\"\n",
    "LOG_PATH = INST_NAME + \"/\"\n",
    "WGT_PREFIX = LOG_PATH+\"weights/\"+INST_NAME\n",
    "if not os.path.exists(LOG_PATH+\"weights\"): os.makedirs(LOG_PATH+\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLQwZi4UOuEw"
   },
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "RpXHxQU2J-J8"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 512  # Remember to run data objects creation on changing this\n",
    "acc_grad = 1\n",
    "learning_rate = 1e-3\n",
    "teacher_forcing, teach_force_till, teach_decay_pereph = 1, 20, 0\n",
    "pretrain_wgt_path = 'Training_2/weights/Training_2_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq2AO5feQADQ"
   },
   "source": [
    "Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "UVP7DQrkQDHh"
   },
   "outputs": [],
   "source": [
    "input_dim = src_glyph.size() + 1\n",
    "output_dim = tgt_glyph.size() + 1\n",
    "enc_emb_dim = 300\n",
    "dec_emb_dim = 300\n",
    "enc_hidden_dim = 512\n",
    "dec_hidden_dim = 512\n",
    "rnn_type = \"lstm\"\n",
    "enc2dec_hid = True\n",
    "attention = True\n",
    "enc_layers = 1\n",
    "dec_layers = 2\n",
    "m_dropout = 0\n",
    "enc_bidirect = True\n",
    "enc_outstate_dim = enc_hidden_dim * (2 if enc_bidirect else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s54j8UNIOHuS"
   },
   "source": [
    "### Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFqWiSX3QfAo"
   },
   "source": [
    "Dataset objects creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "-3R61VwpOP4o"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_file = merge_xlit_jsons([\"data/hindi/HiEn_train1.json\",\n",
    "#                                 \"data/hindi/HiEn_train2.json\" ],\n",
    "#                                 save_prefix= LOG_PATH)\n",
    "\n",
    "train_dataset = XlitData(src_glyph_obj = src_glyph, tgt_glyph_obj = tgt_glyph,\n",
    "                        json_file=TRAIN_FILE, file_map = \"LangEn\",\n",
    "                        padding=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0)\n",
    "\n",
    "# val_dataset = XlitData( src_glyph_obj = src_glyph, tgt_glyph_obj = tgt_glyph,\n",
    "#                         json_file= VALID_FILE, file_map = \"LangEn\",\n",
    "#                         padding=True)\n",
    "\n",
    "\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "#                                 shuffle=True, num_workers=0)\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset.__getitem__(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwym4kg6QlUi"
   },
   "source": [
    "Network Models creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCIgqaq3Q5mN",
    "outputId": "55fe16af-1c5e-49ae-f073-05b859bb72ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained layers: odict_keys(['encoder.embedding.weight', 'encoder.enc_rnn.weight_ih_l0', 'encoder.enc_rnn.weight_hh_l0', 'encoder.enc_rnn.bias_ih_l0', 'encoder.enc_rnn.bias_hh_l0', 'encoder.enc_rnn.weight_ih_l0_reverse', 'encoder.enc_rnn.weight_hh_l0_reverse', 'encoder.enc_rnn.bias_ih_l0_reverse', 'encoder.enc_rnn.bias_hh_l0_reverse', 'decoder.embedding.weight', 'decoder.dec_rnn.weight_ih_l0', 'decoder.dec_rnn.weight_hh_l0', 'decoder.dec_rnn.bias_ih_l0', 'decoder.dec_rnn.bias_hh_l0', 'decoder.dec_rnn.weight_ih_l1', 'decoder.dec_rnn.weight_hh_l1', 'decoder.dec_rnn.bias_ih_l1', 'decoder.dec_rnn.bias_hh_l1', 'decoder.fc.0.weight', 'decoder.fc.0.bias', 'decoder.fc.2.weight', 'decoder.fc.2.bias', 'decoder.W1.weight', 'decoder.W1.bias', 'decoder.W2.weight', 'decoder.W2.bias', 'decoder.V.weight', 'decoder.V.bias'])\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(  input_dim= input_dim, embed_dim = enc_emb_dim,\n",
    "                hidden_dim= enc_hidden_dim,\n",
    "                rnn_type = rnn_type, layers= enc_layers,\n",
    "                dropout= m_dropout, device = device,\n",
    "                bidirectional= enc_bidirect)\n",
    "dec = Decoder(  output_dim= output_dim, embed_dim = dec_emb_dim,\n",
    "                hidden_dim= dec_hidden_dim,\n",
    "                rnn_type = rnn_type, layers= dec_layers,\n",
    "                dropout= m_dropout,\n",
    "                use_attention = attention,\n",
    "                enc_outstate_dim= enc_outstate_dim,\n",
    "                device = device,)\n",
    "\n",
    "model = Seq2Seq(enc, dec, pass_enc2dec_hid=enc2dec_hid,\n",
    "                device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "model = load_pretrained(model, pretrain_wgt_path, device) #if path empty returns unmodified\n",
    "\n",
    "## ----- Load Embeds -----\n",
    "### For Loading charecter embedding from pretrained fasttext model\n",
    "\n",
    "# hi_emb_vecs = np.load(\"hi_char_fasttext.npy\")\n",
    "# model.decoder.embedding.weight.data.copy_(torch.from_numpy(hi_emb_vecs))\n",
    "\n",
    "# en_emb_vecs = np.load(\"en_char_fasttext.npy\")\n",
    "# model.encoder.embedding.weight.data.copy_(torch.from_numpy(en_emb_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UPZ3K_sRgTI",
    "outputId": "cdd50bd2-64ab-4f22-9413-ec9da2c3de8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10180521 trainable parameters\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(58, 300)\n",
      "    (enc_rnn): LSTM(300, 512, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(36, 300)\n",
      "    (dec_rnn): LSTM(1324, 512, num_layers=2, batch_first=True)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=300, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=300, out_features=36, bias=True)\n",
      "    )\n",
      "    (W1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (W2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (V): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "count_train_param(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOcyOudmEAdB"
   },
   "source": [
    "## Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "Isq0bivaEDbF"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "    # weight = torch.from_numpy(train_dataset.tgt_class_weights).to(device)  )  ## For class balancing during training\n",
    "\n",
    "def loss_estimator(pred, truth):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed\n",
    "    pred: batch\n",
    "    \"\"\"\n",
    "    mask = truth.ge(1).type(torch.FloatTensor).to(device)\n",
    "    loss_ = criterion(pred, truth) * mask\n",
    "    return torch.mean(loss_)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=0)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cebSuzeoRc_H"
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNa5OWK0DpaH",
    "outputId": "73b8ac46-6c9c-481a-bd0b-46e1092ebda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/5], MiniBatch-1 loss:0.0152\n",
      "epoch[1/5], MiniBatch-2 loss:0.0152\n",
      "epoch[1/5], MiniBatch-3 loss:0.0155\n",
      "epoch[1/5], MiniBatch-4 loss:0.0151\n",
      "epoch[1/5], MiniBatch-5 loss:0.0156\n",
      "epoch[1/5], MiniBatch-6 loss:0.0156\n",
      "epoch[1/5], MiniBatch-7 loss:0.0154\n",
      "epoch[1/5], MiniBatch-8 loss:0.0154\n",
      "epoch[1/5], MiniBatch-9 loss:0.0156\n",
      "epoch[1/5], MiniBatch-10 loss:0.0158\n",
      "epoch[1/5], MiniBatch-11 loss:0.0151\n",
      "epoch[1/5], MiniBatch-12 loss:0.0154\n",
      "epoch[1/5], MiniBatch-13 loss:0.0154\n",
      "epoch[1/5], MiniBatch-14 loss:0.0159\n",
      "epoch[1/5], MiniBatch-15 loss:0.0158\n",
      "epoch[1/5], MiniBatch-16 loss:0.0158\n",
      "epoch[1/5], MiniBatch-17 loss:0.0156\n",
      "epoch[1/5], MiniBatch-18 loss:0.0159\n",
      "epoch[1/5], MiniBatch-19 loss:0.0154\n",
      "epoch[1/5], MiniBatch-20 loss:0.0158\n",
      "epoch[1/5], MiniBatch-21 loss:0.0153\n",
      "epoch[1/5], MiniBatch-22 loss:0.0157\n",
      "epoch[1/5], MiniBatch-23 loss:0.0161\n",
      "epoch[1/5], MiniBatch-24 loss:0.0154\n",
      "epoch[1/5], MiniBatch-25 loss:0.0156\n",
      "epoch[1/5], MiniBatch-26 loss:0.0151\n",
      "epoch[1/5], MiniBatch-27 loss:0.0157\n",
      "epoch[1/5], MiniBatch-28 loss:0.0155\n",
      "epoch[1/5], MiniBatch-29 loss:0.0156\n",
      "epoch[1/5], MiniBatch-30 loss:0.0159\n",
      "epoch[1/5], MiniBatch-31 loss:0.0152\n",
      "epoch[1/5], MiniBatch-32 loss:0.0157\n",
      "epoch[1/5], MiniBatch-33 loss:0.0156\n",
      "epoch[1/5], MiniBatch-34 loss:0.0157\n",
      "epoch[1/5], MiniBatch-35 loss:0.0159\n",
      "epoch[1/5], MiniBatch-36 loss:0.0156\n",
      "epoch[1/5], MiniBatch-37 loss:0.0161\n",
      "epoch[1/5], MiniBatch-38 loss:0.0156\n",
      "epoch[1/5], MiniBatch-39 loss:0.0158\n",
      "epoch[1/5], MiniBatch-40 loss:0.0160\n",
      "epoch[1/5], MiniBatch-41 loss:0.0158\n",
      "epoch[1/5], MiniBatch-42 loss:0.0157\n",
      "epoch[1/5], MiniBatch-43 loss:0.0158\n",
      "epoch[1/5], MiniBatch-44 loss:0.0152\n",
      "epoch[1/5], MiniBatch-45 loss:0.0159\n",
      "epoch[1/5], MiniBatch-46 loss:0.0157\n",
      "epoch[1/5], MiniBatch-47 loss:0.0155\n",
      "epoch[1/5], MiniBatch-48 loss:0.0159\n",
      "epoch[1/5], MiniBatch-49 loss:0.0156\n",
      "epoch[1/5], MiniBatch-50 loss:0.0153\n",
      "epoch[1/5], MiniBatch-51 loss:0.0152\n",
      "epoch[1/5], MiniBatch-52 loss:0.0157\n",
      "epoch[1/5], MiniBatch-53 loss:0.0152\n",
      "epoch[1/5], MiniBatch-54 loss:0.0156\n",
      "epoch[1/5], MiniBatch-55 loss:0.0155\n",
      "epoch[1/5], MiniBatch-56 loss:0.0154\n",
      "epoch[1/5], MiniBatch-57 loss:0.0154\n",
      "epoch[1/5], MiniBatch-58 loss:0.0155\n",
      "epoch[1/5], MiniBatch-59 loss:0.0155\n",
      "epoch[1/5], MiniBatch-60 loss:0.0161\n",
      "epoch[1/5], MiniBatch-61 loss:0.0157\n",
      "epoch[1/5], MiniBatch-62 loss:0.0159\n",
      "epoch[1/5], MiniBatch-63 loss:0.0156\n",
      "epoch[1/5], MiniBatch-64 loss:0.0153\n",
      "epoch[1/5], MiniBatch-65 loss:0.0159\n",
      "epoch[1/5], MiniBatch-66 loss:0.0156\n",
      "epoch[1/5], MiniBatch-67 loss:0.0156\n",
      "epoch[1/5], MiniBatch-68 loss:0.0157\n",
      "epoch[1/5], MiniBatch-69 loss:0.0156\n",
      "epoch[1/5], MiniBatch-70 loss:0.0160\n",
      "epoch[1/5], MiniBatch-71 loss:0.0154\n",
      "epoch[1/5], MiniBatch-72 loss:0.0155\n",
      "epoch[1/5], MiniBatch-73 loss:0.0157\n",
      "epoch[1/5], MiniBatch-74 loss:0.0158\n",
      "epoch[1/5], MiniBatch-75 loss:0.0158\n",
      "epoch[1/5], MiniBatch-76 loss:0.0155\n",
      "epoch[1/5], MiniBatch-77 loss:0.0156\n",
      "epoch[1/5], MiniBatch-78 loss:0.0158\n",
      "epoch[1/5], MiniBatch-79 loss:0.0155\n",
      "epoch[1/5], MiniBatch-80 loss:0.0153\n",
      "epoch[1/5], MiniBatch-81 loss:0.0158\n",
      "epoch[1/5], MiniBatch-82 loss:0.0155\n",
      "epoch[1/5], MiniBatch-83 loss:0.0154\n",
      "epoch[1/5], MiniBatch-84 loss:0.0150\n",
      "epoch[1/5], MiniBatch-85 loss:0.0156\n",
      "epoch[1/5], MiniBatch-86 loss:0.0157\n",
      "epoch[1/5], MiniBatch-87 loss:0.0156\n",
      "epoch[1/5], MiniBatch-88 loss:0.0159\n",
      "epoch[1/5], MiniBatch-89 loss:0.0154\n",
      "epoch[1/5], MiniBatch-90 loss:0.0151\n",
      "epoch[1/5], MiniBatch-91 loss:0.0159\n",
      "epoch[1/5], MiniBatch-92 loss:0.0155\n",
      "epoch[1/5], MiniBatch-93 loss:0.0157\n",
      "epoch[1/5], MiniBatch-94 loss:0.0158\n",
      "epoch[1/5], MiniBatch-95 loss:0.0155\n",
      "epoch[1/5], MiniBatch-96 loss:0.0152\n",
      "epoch[1/5], MiniBatch-97 loss:0.0158\n",
      "epoch[1/5], MiniBatch-98 loss:0.0155\n",
      "epoch[1/5], MiniBatch-99 loss:0.0154\n",
      "epoch[1/5], MiniBatch-100 loss:0.0158\n",
      "epoch[1/5], MiniBatch-101 loss:0.0157\n",
      "epoch[1/5], MiniBatch-102 loss:0.0155\n",
      "epoch[1/5], MiniBatch-103 loss:0.0157\n",
      "epoch[1/5], MiniBatch-104 loss:0.0157\n",
      "epoch[1/5], MiniBatch-105 loss:0.0159\n",
      "epoch[1/5], MiniBatch-106 loss:0.0158\n",
      "epoch[1/5], MiniBatch-107 loss:0.0154\n",
      "epoch[1/5], MiniBatch-108 loss:0.0155\n",
      "epoch[1/5], MiniBatch-109 loss:0.0161\n",
      "epoch[1/5], MiniBatch-110 loss:0.0156\n",
      "epoch[1/5], MiniBatch-111 loss:0.0156\n",
      "epoch[1/5], MiniBatch-112 loss:0.0157\n",
      "epoch[1/5], MiniBatch-113 loss:0.0157\n",
      "epoch[1/5], MiniBatch-114 loss:0.0154\n",
      "epoch[1/5], MiniBatch-115 loss:0.0161\n",
      "epoch[1/5], MiniBatch-116 loss:0.0158\n",
      "epoch[1/5], MiniBatch-117 loss:0.0153\n",
      "epoch[1/5], MiniBatch-118 loss:0.0159\n",
      "epoch[1/5], MiniBatch-119 loss:0.0156\n",
      "epoch[1/5], MiniBatch-120 loss:0.0160\n",
      "epoch[1/5], MiniBatch-121 loss:0.0156\n",
      "epoch[1/5], MiniBatch-122 loss:0.0154\n",
      "epoch[1/5], MiniBatch-123 loss:0.0155\n",
      "epoch[1/5], MiniBatch-124 loss:0.0156\n",
      "epoch[1/5], MiniBatch-125 loss:0.0151\n",
      "epoch[1/5], MiniBatch-126 loss:0.0156\n",
      "epoch[1/5], MiniBatch-127 loss:0.0160\n",
      "epoch[1/5], MiniBatch-128 loss:0.0154\n",
      "epoch[1/5], MiniBatch-129 loss:0.0158\n",
      "epoch[1/5], MiniBatch-130 loss:0.0158\n",
      "epoch[1/5], MiniBatch-131 loss:0.0156\n",
      "epoch[1/5], MiniBatch-132 loss:0.0158\n",
      "epoch[1/5], MiniBatch-133 loss:0.0159\n",
      "epoch[1/5], MiniBatch-134 loss:0.0157\n",
      "epoch[1/5], MiniBatch-135 loss:0.0157\n",
      "epoch[1/5], MiniBatch-136 loss:0.0157\n",
      "epoch[1/5], MiniBatch-137 loss:0.0153\n",
      "epoch[1/5], MiniBatch-138 loss:0.0160\n",
      "epoch[1/5], MiniBatch-139 loss:0.0158\n",
      "epoch[1/5], MiniBatch-140 loss:0.0155\n",
      "epoch[1/5], MiniBatch-141 loss:0.0160\n",
      "epoch[1/5], MiniBatch-142 loss:0.0160\n",
      "epoch[1/5], MiniBatch-143 loss:0.0155\n",
      "epoch[1/5], MiniBatch-144 loss:0.0159\n",
      "epoch[1/5], MiniBatch-145 loss:0.0161\n",
      "epoch[1/5], MiniBatch-146 loss:0.0156\n",
      "epoch[1/5], MiniBatch-147 loss:0.0156\n",
      "epoch[1/5], MiniBatch-148 loss:0.0159\n",
      "epoch[1/5], MiniBatch-149 loss:0.0156\n",
      "epoch[1/5], MiniBatch-150 loss:0.0162\n",
      "epoch[1/5], MiniBatch-151 loss:0.0155\n",
      "epoch[1/5], MiniBatch-152 loss:0.0156\n",
      "epoch[1/5], MiniBatch-153 loss:0.0163\n",
      "epoch[1/5], MiniBatch-154 loss:0.0156\n",
      "epoch[1/5], MiniBatch-155 loss:0.0157\n",
      "epoch[1/5], MiniBatch-156 loss:0.0155\n",
      "epoch[1/5], MiniBatch-157 loss:0.0156\n",
      "epoch[1/5], MiniBatch-158 loss:0.0160\n",
      "epoch[1/5], MiniBatch-159 loss:0.0156\n",
      "epoch[1/5], MiniBatch-160 loss:0.0157\n",
      "epoch[1/5], MiniBatch-161 loss:0.0156\n",
      "epoch[1/5], MiniBatch-162 loss:0.0162\n",
      "epoch[1/5], MiniBatch-163 loss:0.0156\n",
      "epoch[1/5], MiniBatch-164 loss:0.0157\n",
      "epoch[1/5], MiniBatch-165 loss:0.0154\n",
      "epoch[1/5], MiniBatch-166 loss:0.0158\n",
      "epoch[1/5], MiniBatch-167 loss:0.0157\n",
      "epoch[1/5], MiniBatch-168 loss:0.0158\n",
      "epoch[1/5], MiniBatch-169 loss:0.0156\n",
      "epoch[1/5], MiniBatch-170 loss:0.0157\n",
      "epoch[1/5], MiniBatch-171 loss:0.0160\n",
      "epoch[1/5], MiniBatch-172 loss:0.0156\n",
      "epoch[1/5], MiniBatch-173 loss:0.0160\n",
      "epoch[1/5], MiniBatch-174 loss:0.0152\n",
      "epoch[1/5], MiniBatch-175 loss:0.0156\n",
      "epoch[1/5], MiniBatch-176 loss:0.0153\n",
      "epoch[1/5], MiniBatch-177 loss:0.0156\n",
      "epoch[1/5], MiniBatch-178 loss:0.0154\n",
      "epoch[1/5], MiniBatch-179 loss:0.0156\n",
      "epoch[1/5], MiniBatch-180 loss:0.0155\n",
      "epoch[1/5], MiniBatch-181 loss:0.0158\n",
      "epoch[1/5], MiniBatch-182 loss:0.0155\n",
      "epoch[1/5], MiniBatch-183 loss:0.0157\n",
      "epoch[1/5], MiniBatch-184 loss:0.0160\n",
      "epoch[1/5], MiniBatch-185 loss:0.0159\n",
      "epoch[1/5], MiniBatch-186 loss:0.0160\n",
      "epoch[1/5], MiniBatch-187 loss:0.0161\n",
      "epoch[1/5], MiniBatch-188 loss:0.0158\n",
      "epoch[1/5], MiniBatch-189 loss:0.0159\n",
      "epoch[1/5], MiniBatch-190 loss:0.0157\n",
      "epoch[1/5], MiniBatch-191 loss:0.0160\n",
      "epoch[1/5], MiniBatch-192 loss:0.0157\n",
      "epoch[1/5], MiniBatch-193 loss:0.0157\n",
      "epoch[1/5], MiniBatch-194 loss:0.0159\n",
      "epoch[1/5], MiniBatch-195 loss:0.0159\n",
      "epoch[1/5], MiniBatch-196 loss:0.0157\n",
      "epoch[1/5], MiniBatch-197 loss:0.0156\n",
      "epoch[1/5], MiniBatch-198 loss:0.0162\n",
      "epoch[1/5], MiniBatch-199 loss:0.0160\n",
      "epoch[1/5], MiniBatch-200 loss:0.0157\n",
      "epoch[1/5], MiniBatch-201 loss:0.0152\n",
      "epoch[1/5], MiniBatch-202 loss:0.0160\n",
      "epoch[1/5], MiniBatch-203 loss:0.0155\n",
      "epoch[1/5], MiniBatch-204 loss:0.0157\n",
      "epoch[1/5], MiniBatch-205 loss:0.0156\n",
      "epoch[1/5], MiniBatch-206 loss:0.0160\n",
      "epoch[1/5], MiniBatch-207 loss:0.0157\n",
      "epoch[1/5], MiniBatch-208 loss:0.0155\n",
      "epoch[1/5], MiniBatch-209 loss:0.0160\n",
      "epoch[1/5], MiniBatch-210 loss:0.0155\n",
      "epoch[1/5], MiniBatch-211 loss:0.0158\n",
      "epoch[1/5], MiniBatch-212 loss:0.0157\n",
      "epoch[1/5], MiniBatch-213 loss:0.0155\n",
      "epoch[1/5], MiniBatch-214 loss:0.0160\n",
      "epoch[1/5], MiniBatch-215 loss:0.0156\n",
      "epoch[1/5], MiniBatch-216 loss:0.0154\n",
      "epoch[1/5], MiniBatch-217 loss:0.0158\n",
      "epoch[1/5], MiniBatch-218 loss:0.0161\n",
      "epoch[1/5], MiniBatch-219 loss:0.0155\n",
      "epoch[1/5], MiniBatch-220 loss:0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/5], MiniBatch-221 loss:0.0153\n",
      "epoch[1/5], MiniBatch-222 loss:0.0155\n",
      "epoch[1/5], MiniBatch-223 loss:0.0157\n",
      "epoch[1/5], MiniBatch-224 loss:0.0151\n",
      "epoch[1/5], MiniBatch-225 loss:0.0156\n",
      "epoch[1/5], MiniBatch-226 loss:0.0158\n",
      "epoch[1/5], MiniBatch-227 loss:0.0159\n",
      "epoch[1/5], MiniBatch-228 loss:0.0162\n",
      "epoch[1/5], MiniBatch-229 loss:0.0155\n",
      "epoch[1/5], MiniBatch-230 loss:0.0160\n",
      "epoch[1/5], MiniBatch-231 loss:0.0157\n",
      "epoch[1/5], MiniBatch-232 loss:0.0163\n",
      "epoch[1/5], MiniBatch-233 loss:0.0156\n",
      "epoch[1/5], MiniBatch-234 loss:0.0155\n",
      "epoch[1/5], MiniBatch-235 loss:0.0155\n",
      "epoch[1/5], MiniBatch-236 loss:0.0161\n",
      "epoch[1/5], MiniBatch-237 loss:0.0155\n",
      "epoch[1/5], MiniBatch-238 loss:0.0162\n",
      "epoch[1/5], MiniBatch-239 loss:0.0158\n",
      "epoch[1/5], MiniBatch-240 loss:0.0159\n",
      "epoch[1/5], MiniBatch-241 loss:0.0159\n",
      "epoch[1/5], MiniBatch-242 loss:0.0157\n",
      "epoch[1/5], MiniBatch-243 loss:0.0158\n",
      "epoch[1/5], MiniBatch-244 loss:0.0161\n",
      "epoch[1/5], MiniBatch-245 loss:0.0159\n",
      "epoch[1/5], MiniBatch-246 loss:0.0161\n",
      "epoch[1/5], MiniBatch-247 loss:0.0159\n",
      "epoch[1/5], MiniBatch-248 loss:0.0157\n",
      "epoch[1/5], MiniBatch-249 loss:0.0156\n",
      "epoch[1/5], MiniBatch-250 loss:0.0157\n",
      "epoch[1/5], MiniBatch-251 loss:0.0159\n",
      "epoch[1/5], MiniBatch-252 loss:0.0159\n",
      "epoch[1/5], MiniBatch-253 loss:0.0162\n",
      "epoch[1/5], MiniBatch-254 loss:0.0162\n",
      "epoch[1/5], MiniBatch-255 loss:0.0159\n",
      "epoch[1/5], MiniBatch-256 loss:0.0160\n",
      "epoch[1/5], MiniBatch-257 loss:0.0162\n",
      "epoch[1/5], MiniBatch-258 loss:0.0156\n",
      "epoch[1/5], MiniBatch-259 loss:0.0158\n",
      "epoch[1/5], MiniBatch-260 loss:0.0162\n",
      "epoch[1/5], MiniBatch-261 loss:0.0156\n",
      "epoch[1/5], MiniBatch-262 loss:0.0163\n",
      "epoch[1/5], MiniBatch-263 loss:0.0159\n",
      "epoch[1/5], MiniBatch-264 loss:0.0162\n",
      "epoch[1/5], MiniBatch-265 loss:0.0159\n",
      "epoch[1/5], MiniBatch-266 loss:0.0160\n",
      "epoch[1/5], MiniBatch-267 loss:0.0158\n",
      "epoch[1/5], MiniBatch-268 loss:0.0162\n",
      "epoch[1/5], MiniBatch-269 loss:0.0157\n",
      "epoch[1/5], MiniBatch-270 loss:0.0162\n",
      "epoch[1/5], MiniBatch-271 loss:0.0164\n",
      "epoch[1/5], MiniBatch-272 loss:0.0160\n",
      "epoch[1/5], MiniBatch-273 loss:0.0160\n",
      "epoch[1/5], MiniBatch-274 loss:0.0160\n",
      "epoch[1/5], MiniBatch-275 loss:0.0155\n",
      "epoch[1/5], MiniBatch-276 loss:0.0156\n",
      "epoch[1/5], MiniBatch-277 loss:0.0161\n",
      "epoch[1/5], MiniBatch-278 loss:0.0158\n",
      "epoch[1/5], MiniBatch-279 loss:0.0163\n",
      "epoch[1/5], MiniBatch-280 loss:0.0159\n",
      "epoch[1/5], MiniBatch-281 loss:0.0159\n",
      "epoch[1/5], MiniBatch-282 loss:0.0156\n",
      "epoch[1/5], MiniBatch-283 loss:0.0156\n",
      "epoch[1/5], MiniBatch-284 loss:0.0155\n",
      "epoch[1/5], MiniBatch-285 loss:0.0158\n",
      "epoch[1/5], MiniBatch-286 loss:0.0160\n",
      "epoch[1/5], MiniBatch-287 loss:0.0159\n",
      "epoch[1/5], MiniBatch-288 loss:0.0158\n",
      "epoch[1/5], MiniBatch-289 loss:0.0161\n",
      "epoch[1/5], MiniBatch-290 loss:0.0161\n",
      "epoch[1/5], MiniBatch-291 loss:0.0158\n",
      "epoch[1/5], MiniBatch-292 loss:0.0162\n",
      "epoch[1/5], MiniBatch-293 loss:0.0154\n",
      "epoch[1/5], MiniBatch-294 loss:0.0163\n",
      "epoch[1/5], MiniBatch-295 loss:0.0157\n",
      "epoch[1/5], MiniBatch-296 loss:0.0162\n",
      "epoch[1/5], MiniBatch-297 loss:0.0163\n",
      "epoch[1/5], MiniBatch-298 loss:0.0158\n",
      "epoch[1/5], MiniBatch-299 loss:0.0161\n",
      "epoch[1/5], MiniBatch-300 loss:0.0160\n",
      "epoch[1/5], MiniBatch-301 loss:0.0159\n",
      "epoch[1/5], MiniBatch-302 loss:0.0158\n",
      "epoch[1/5], MiniBatch-303 loss:0.0159\n",
      "epoch[1/5], MiniBatch-304 loss:0.0160\n",
      "epoch[1/5], MiniBatch-305 loss:0.0162\n",
      "epoch[1/5], MiniBatch-306 loss:0.0158\n",
      "epoch[1/5], MiniBatch-307 loss:0.0155\n",
      "epoch[1/5], MiniBatch-308 loss:0.0158\n",
      "epoch[1/5], MiniBatch-309 loss:0.0160\n",
      "epoch[1/5], MiniBatch-310 loss:0.0165\n",
      "epoch[1/5], MiniBatch-311 loss:0.0157\n",
      "epoch[1/5], MiniBatch-312 loss:0.0159\n",
      "epoch[1/5], MiniBatch-313 loss:0.0164\n",
      "epoch[1/5], MiniBatch-314 loss:0.0161\n",
      "epoch[1/5], MiniBatch-315 loss:0.0162\n",
      "epoch[1/5], MiniBatch-316 loss:0.0164\n",
      "epoch[1/5], MiniBatch-317 loss:0.0158\n",
      "epoch[1/5], MiniBatch-318 loss:0.0160\n",
      "epoch[1/5], MiniBatch-319 loss:0.0159\n",
      "epoch[1/5], MiniBatch-320 loss:0.0161\n",
      "epoch[1/5], MiniBatch-321 loss:0.0160\n",
      "epoch[1/5], MiniBatch-322 loss:0.0158\n",
      "epoch[1/5], MiniBatch-323 loss:0.0162\n",
      "epoch[1/5], MiniBatch-324 loss:0.0155\n",
      "epoch[1/5], MiniBatch-325 loss:0.0155\n",
      "epoch[1/5], MiniBatch-326 loss:0.0159\n",
      "epoch[1/5], MiniBatch-327 loss:0.0157\n",
      "epoch[1/5], MiniBatch-328 loss:0.0156\n",
      "epoch[1/5], MiniBatch-329 loss:0.0157\n",
      "epoch[1/5], MiniBatch-330 loss:0.0156\n",
      "epoch[1/5], MiniBatch-331 loss:0.0157\n",
      "epoch[1/5], MiniBatch-332 loss:0.0161\n",
      "epoch[1/5], MiniBatch-333 loss:0.0157\n",
      "epoch[1/5], MiniBatch-334 loss:0.0156\n",
      "epoch[1/5], MiniBatch-335 loss:0.0156\n",
      "epoch[1/5], MiniBatch-336 loss:0.0160\n",
      "epoch[1/5], MiniBatch-337 loss:0.0160\n",
      "epoch[1/5], MiniBatch-338 loss:0.0160\n",
      "epoch[1/5], MiniBatch-339 loss:0.0161\n",
      "epoch[1/5], MiniBatch-340 loss:0.0159\n",
      "epoch[1/5], MiniBatch-341 loss:0.0158\n",
      "epoch[1/5], MiniBatch-342 loss:0.0158\n",
      "epoch[1/5], MiniBatch-343 loss:0.0160\n",
      "epoch[1/5], MiniBatch-344 loss:0.0161\n",
      "epoch[1/5], MiniBatch-345 loss:0.0156\n",
      "epoch[1/5], MiniBatch-346 loss:0.0159\n",
      "epoch[1/5], MiniBatch-347 loss:0.0158\n",
      "epoch[1/5], MiniBatch-348 loss:0.0162\n",
      "epoch[1/5], MiniBatch-349 loss:0.0156\n",
      "epoch[1/5], MiniBatch-350 loss:0.0158\n",
      "epoch[1/5], MiniBatch-351 loss:0.0165\n",
      "epoch[1/5], MiniBatch-352 loss:0.0159\n",
      "epoch[1/5], MiniBatch-353 loss:0.0160\n",
      "epoch[1/5], MiniBatch-354 loss:0.0161\n",
      "epoch[1/5], MiniBatch-355 loss:0.0160\n",
      "epoch[1/5], MiniBatch-356 loss:0.0164\n",
      "epoch[1/5], MiniBatch-357 loss:0.0159\n",
      "epoch[1/5], MiniBatch-358 loss:0.0161\n",
      "epoch[1/5], MiniBatch-359 loss:0.0162\n",
      "epoch[1/5], MiniBatch-360 loss:0.0159\n",
      "epoch[1/5], MiniBatch-361 loss:0.0159\n",
      "epoch[1/5], MiniBatch-362 loss:0.0158\n",
      "epoch[1/5], MiniBatch-363 loss:0.0161\n",
      "epoch[1/5], MiniBatch-364 loss:0.0160\n",
      "epoch[1/5], MiniBatch-365 loss:0.0160\n",
      "epoch[1/5], MiniBatch-366 loss:0.0159\n",
      "epoch[1/5], MiniBatch-367 loss:0.0160\n",
      "epoch[1/5], MiniBatch-368 loss:0.0156\n",
      "epoch[1/5], MiniBatch-369 loss:0.0161\n",
      "epoch[1/5], MiniBatch-370 loss:0.0162\n",
      "epoch[1/5], MiniBatch-371 loss:0.0162\n",
      "epoch[1/5], MiniBatch-372 loss:0.0159\n",
      "epoch[1/5], MiniBatch-373 loss:0.0159\n",
      "epoch[1/5], MiniBatch-374 loss:0.0162\n",
      "epoch[1/5], MiniBatch-375 loss:0.0158\n",
      "epoch[1/5], MiniBatch-376 loss:0.0159\n",
      "epoch[1/5], MiniBatch-377 loss:0.0162\n",
      "epoch[1/5], MiniBatch-378 loss:0.0157\n",
      "epoch[1/5], MiniBatch-379 loss:0.0161\n",
      "epoch[1/5], MiniBatch-380 loss:0.0159\n",
      "epoch[1/5], MiniBatch-381 loss:0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:42<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/5], [-----TEST------] loss:0.0339  Accur:0.9260\n",
      "***saving best optimal state [Loss:0.033886540681123734 Accur:0.92596435546875] ***\n",
      "epoch[2/5], MiniBatch-1 loss:0.0155\n",
      "epoch[2/5], MiniBatch-2 loss:0.0155\n",
      "epoch[2/5], MiniBatch-3 loss:0.0156\n",
      "epoch[2/5], MiniBatch-4 loss:0.0155\n",
      "epoch[2/5], MiniBatch-5 loss:0.0156\n",
      "epoch[2/5], MiniBatch-6 loss:0.0154\n",
      "epoch[2/5], MiniBatch-7 loss:0.0156\n",
      "epoch[2/5], MiniBatch-8 loss:0.0155\n",
      "epoch[2/5], MiniBatch-9 loss:0.0157\n",
      "epoch[2/5], MiniBatch-10 loss:0.0153\n",
      "epoch[2/5], MiniBatch-11 loss:0.0157\n",
      "epoch[2/5], MiniBatch-12 loss:0.0153\n",
      "epoch[2/5], MiniBatch-13 loss:0.0150\n",
      "epoch[2/5], MiniBatch-14 loss:0.0153\n",
      "epoch[2/5], MiniBatch-15 loss:0.0158\n",
      "epoch[2/5], MiniBatch-16 loss:0.0156\n",
      "epoch[2/5], MiniBatch-17 loss:0.0155\n",
      "epoch[2/5], MiniBatch-18 loss:0.0149\n",
      "epoch[2/5], MiniBatch-19 loss:0.0153\n",
      "epoch[2/5], MiniBatch-20 loss:0.0151\n",
      "epoch[2/5], MiniBatch-21 loss:0.0153\n",
      "epoch[2/5], MiniBatch-22 loss:0.0157\n",
      "epoch[2/5], MiniBatch-23 loss:0.0156\n",
      "epoch[2/5], MiniBatch-24 loss:0.0156\n",
      "epoch[2/5], MiniBatch-25 loss:0.0156\n",
      "epoch[2/5], MiniBatch-26 loss:0.0153\n",
      "epoch[2/5], MiniBatch-27 loss:0.0155\n",
      "epoch[2/5], MiniBatch-28 loss:0.0154\n",
      "epoch[2/5], MiniBatch-29 loss:0.0152\n",
      "epoch[2/5], MiniBatch-30 loss:0.0150\n",
      "epoch[2/5], MiniBatch-31 loss:0.0151\n",
      "epoch[2/5], MiniBatch-32 loss:0.0151\n",
      "epoch[2/5], MiniBatch-33 loss:0.0156\n",
      "epoch[2/5], MiniBatch-34 loss:0.0156\n",
      "epoch[2/5], MiniBatch-35 loss:0.0153\n",
      "epoch[2/5], MiniBatch-36 loss:0.0153\n",
      "epoch[2/5], MiniBatch-37 loss:0.0153\n",
      "epoch[2/5], MiniBatch-38 loss:0.0153\n",
      "epoch[2/5], MiniBatch-39 loss:0.0153\n",
      "epoch[2/5], MiniBatch-40 loss:0.0153\n",
      "epoch[2/5], MiniBatch-41 loss:0.0154\n",
      "epoch[2/5], MiniBatch-42 loss:0.0153\n",
      "epoch[2/5], MiniBatch-43 loss:0.0150\n",
      "epoch[2/5], MiniBatch-44 loss:0.0150\n",
      "epoch[2/5], MiniBatch-45 loss:0.0157\n",
      "epoch[2/5], MiniBatch-46 loss:0.0152\n",
      "epoch[2/5], MiniBatch-47 loss:0.0151\n",
      "epoch[2/5], MiniBatch-48 loss:0.0159\n",
      "epoch[2/5], MiniBatch-49 loss:0.0155\n",
      "epoch[2/5], MiniBatch-50 loss:0.0154\n",
      "epoch[2/5], MiniBatch-51 loss:0.0153\n",
      "epoch[2/5], MiniBatch-52 loss:0.0154\n",
      "epoch[2/5], MiniBatch-53 loss:0.0158\n",
      "epoch[2/5], MiniBatch-54 loss:0.0157\n",
      "epoch[2/5], MiniBatch-55 loss:0.0153\n",
      "epoch[2/5], MiniBatch-56 loss:0.0155\n",
      "epoch[2/5], MiniBatch-57 loss:0.0153\n",
      "epoch[2/5], MiniBatch-58 loss:0.0150\n",
      "epoch[2/5], MiniBatch-59 loss:0.0152\n",
      "epoch[2/5], MiniBatch-60 loss:0.0152\n",
      "epoch[2/5], MiniBatch-61 loss:0.0156\n",
      "epoch[2/5], MiniBatch-62 loss:0.0154\n",
      "epoch[2/5], MiniBatch-63 loss:0.0157\n",
      "epoch[2/5], MiniBatch-64 loss:0.0155\n",
      "epoch[2/5], MiniBatch-65 loss:0.0153\n",
      "epoch[2/5], MiniBatch-66 loss:0.0152\n",
      "epoch[2/5], MiniBatch-67 loss:0.0157\n",
      "epoch[2/5], MiniBatch-68 loss:0.0155\n",
      "epoch[2/5], MiniBatch-69 loss:0.0151\n",
      "epoch[2/5], MiniBatch-70 loss:0.0157\n",
      "epoch[2/5], MiniBatch-71 loss:0.0155\n",
      "epoch[2/5], MiniBatch-72 loss:0.0155\n",
      "epoch[2/5], MiniBatch-73 loss:0.0157\n",
      "epoch[2/5], MiniBatch-74 loss:0.0154\n",
      "epoch[2/5], MiniBatch-75 loss:0.0157\n",
      "epoch[2/5], MiniBatch-76 loss:0.0156\n",
      "epoch[2/5], MiniBatch-77 loss:0.0155\n",
      "epoch[2/5], MiniBatch-78 loss:0.0160\n",
      "epoch[2/5], MiniBatch-79 loss:0.0158\n",
      "epoch[2/5], MiniBatch-80 loss:0.0156\n",
      "epoch[2/5], MiniBatch-81 loss:0.0156\n",
      "epoch[2/5], MiniBatch-82 loss:0.0153\n",
      "epoch[2/5], MiniBatch-83 loss:0.0153\n",
      "epoch[2/5], MiniBatch-84 loss:0.0155\n",
      "epoch[2/5], MiniBatch-85 loss:0.0153\n",
      "epoch[2/5], MiniBatch-86 loss:0.0158\n",
      "epoch[2/5], MiniBatch-87 loss:0.0155\n",
      "epoch[2/5], MiniBatch-88 loss:0.0156\n",
      "epoch[2/5], MiniBatch-89 loss:0.0153\n",
      "epoch[2/5], MiniBatch-90 loss:0.0153\n",
      "epoch[2/5], MiniBatch-91 loss:0.0156\n",
      "epoch[2/5], MiniBatch-92 loss:0.0156\n",
      "epoch[2/5], MiniBatch-93 loss:0.0153\n",
      "epoch[2/5], MiniBatch-94 loss:0.0152\n",
      "epoch[2/5], MiniBatch-95 loss:0.0156\n",
      "epoch[2/5], MiniBatch-96 loss:0.0152\n",
      "epoch[2/5], MiniBatch-97 loss:0.0153\n",
      "epoch[2/5], MiniBatch-98 loss:0.0154\n",
      "epoch[2/5], MiniBatch-99 loss:0.0154\n",
      "epoch[2/5], MiniBatch-100 loss:0.0150\n",
      "epoch[2/5], MiniBatch-101 loss:0.0156\n",
      "epoch[2/5], MiniBatch-102 loss:0.0152\n",
      "epoch[2/5], MiniBatch-103 loss:0.0153\n",
      "epoch[2/5], MiniBatch-104 loss:0.0150\n",
      "epoch[2/5], MiniBatch-105 loss:0.0160\n",
      "epoch[2/5], MiniBatch-106 loss:0.0156\n",
      "epoch[2/5], MiniBatch-107 loss:0.0155\n",
      "epoch[2/5], MiniBatch-108 loss:0.0154\n",
      "epoch[2/5], MiniBatch-109 loss:0.0155\n",
      "epoch[2/5], MiniBatch-110 loss:0.0157\n",
      "epoch[2/5], MiniBatch-111 loss:0.0154\n",
      "epoch[2/5], MiniBatch-112 loss:0.0156\n",
      "epoch[2/5], MiniBatch-113 loss:0.0152\n",
      "epoch[2/5], MiniBatch-114 loss:0.0153\n",
      "epoch[2/5], MiniBatch-115 loss:0.0159\n",
      "epoch[2/5], MiniBatch-116 loss:0.0158\n",
      "epoch[2/5], MiniBatch-117 loss:0.0157\n",
      "epoch[2/5], MiniBatch-118 loss:0.0155\n",
      "epoch[2/5], MiniBatch-119 loss:0.0154\n",
      "epoch[2/5], MiniBatch-120 loss:0.0157\n",
      "epoch[2/5], MiniBatch-121 loss:0.0155\n",
      "epoch[2/5], MiniBatch-122 loss:0.0156\n",
      "epoch[2/5], MiniBatch-123 loss:0.0155\n",
      "epoch[2/5], MiniBatch-124 loss:0.0154\n",
      "epoch[2/5], MiniBatch-125 loss:0.0158\n",
      "epoch[2/5], MiniBatch-126 loss:0.0153\n",
      "epoch[2/5], MiniBatch-127 loss:0.0157\n",
      "epoch[2/5], MiniBatch-128 loss:0.0157\n",
      "epoch[2/5], MiniBatch-129 loss:0.0153\n",
      "epoch[2/5], MiniBatch-130 loss:0.0156\n",
      "epoch[2/5], MiniBatch-131 loss:0.0157\n",
      "epoch[2/5], MiniBatch-132 loss:0.0153\n",
      "epoch[2/5], MiniBatch-133 loss:0.0154\n",
      "epoch[2/5], MiniBatch-134 loss:0.0155\n",
      "epoch[2/5], MiniBatch-135 loss:0.0154\n",
      "epoch[2/5], MiniBatch-136 loss:0.0157\n",
      "epoch[2/5], MiniBatch-137 loss:0.0157\n",
      "epoch[2/5], MiniBatch-138 loss:0.0157\n",
      "epoch[2/5], MiniBatch-139 loss:0.0156\n",
      "epoch[2/5], MiniBatch-140 loss:0.0158\n",
      "epoch[2/5], MiniBatch-141 loss:0.0161\n",
      "epoch[2/5], MiniBatch-142 loss:0.0159\n",
      "epoch[2/5], MiniBatch-143 loss:0.0154\n",
      "epoch[2/5], MiniBatch-144 loss:0.0157\n",
      "epoch[2/5], MiniBatch-145 loss:0.0158\n",
      "epoch[2/5], MiniBatch-146 loss:0.0157\n",
      "epoch[2/5], MiniBatch-147 loss:0.0157\n",
      "epoch[2/5], MiniBatch-148 loss:0.0159\n",
      "epoch[2/5], MiniBatch-149 loss:0.0157\n",
      "epoch[2/5], MiniBatch-150 loss:0.0158\n",
      "epoch[2/5], MiniBatch-151 loss:0.0156\n",
      "epoch[2/5], MiniBatch-152 loss:0.0157\n",
      "epoch[2/5], MiniBatch-153 loss:0.0158\n",
      "epoch[2/5], MiniBatch-154 loss:0.0152\n",
      "epoch[2/5], MiniBatch-155 loss:0.0157\n",
      "epoch[2/5], MiniBatch-156 loss:0.0156\n",
      "epoch[2/5], MiniBatch-157 loss:0.0156\n",
      "epoch[2/5], MiniBatch-158 loss:0.0160\n",
      "epoch[2/5], MiniBatch-159 loss:0.0156\n",
      "epoch[2/5], MiniBatch-160 loss:0.0161\n",
      "epoch[2/5], MiniBatch-161 loss:0.0157\n",
      "epoch[2/5], MiniBatch-162 loss:0.0159\n",
      "epoch[2/5], MiniBatch-163 loss:0.0156\n",
      "epoch[2/5], MiniBatch-164 loss:0.0159\n",
      "epoch[2/5], MiniBatch-165 loss:0.0154\n",
      "epoch[2/5], MiniBatch-166 loss:0.0156\n",
      "epoch[2/5], MiniBatch-167 loss:0.0155\n",
      "epoch[2/5], MiniBatch-168 loss:0.0155\n",
      "epoch[2/5], MiniBatch-169 loss:0.0157\n",
      "epoch[2/5], MiniBatch-170 loss:0.0162\n",
      "epoch[2/5], MiniBatch-171 loss:0.0159\n",
      "epoch[2/5], MiniBatch-172 loss:0.0158\n",
      "epoch[2/5], MiniBatch-173 loss:0.0156\n",
      "epoch[2/5], MiniBatch-174 loss:0.0154\n",
      "epoch[2/5], MiniBatch-175 loss:0.0157\n",
      "epoch[2/5], MiniBatch-176 loss:0.0158\n",
      "epoch[2/5], MiniBatch-177 loss:0.0159\n",
      "epoch[2/5], MiniBatch-178 loss:0.0157\n",
      "epoch[2/5], MiniBatch-179 loss:0.0157\n",
      "epoch[2/5], MiniBatch-180 loss:0.0157\n",
      "epoch[2/5], MiniBatch-181 loss:0.0156\n",
      "epoch[2/5], MiniBatch-182 loss:0.0160\n",
      "epoch[2/5], MiniBatch-183 loss:0.0161\n",
      "epoch[2/5], MiniBatch-184 loss:0.0155\n",
      "epoch[2/5], MiniBatch-185 loss:0.0155\n",
      "epoch[2/5], MiniBatch-186 loss:0.0157\n",
      "epoch[2/5], MiniBatch-187 loss:0.0155\n",
      "epoch[2/5], MiniBatch-188 loss:0.0156\n",
      "epoch[2/5], MiniBatch-189 loss:0.0154\n",
      "epoch[2/5], MiniBatch-190 loss:0.0158\n",
      "epoch[2/5], MiniBatch-191 loss:0.0156\n",
      "epoch[2/5], MiniBatch-192 loss:0.0160\n",
      "epoch[2/5], MiniBatch-193 loss:0.0154\n",
      "epoch[2/5], MiniBatch-194 loss:0.0158\n",
      "epoch[2/5], MiniBatch-195 loss:0.0153\n",
      "epoch[2/5], MiniBatch-196 loss:0.0154\n",
      "epoch[2/5], MiniBatch-197 loss:0.0158\n",
      "epoch[2/5], MiniBatch-198 loss:0.0155\n",
      "epoch[2/5], MiniBatch-199 loss:0.0156\n",
      "epoch[2/5], MiniBatch-200 loss:0.0159\n",
      "epoch[2/5], MiniBatch-201 loss:0.0157\n",
      "epoch[2/5], MiniBatch-202 loss:0.0159\n",
      "epoch[2/5], MiniBatch-203 loss:0.0156\n",
      "epoch[2/5], MiniBatch-204 loss:0.0154\n",
      "epoch[2/5], MiniBatch-205 loss:0.0156\n",
      "epoch[2/5], MiniBatch-206 loss:0.0154\n",
      "epoch[2/5], MiniBatch-207 loss:0.0154\n",
      "epoch[2/5], MiniBatch-208 loss:0.0160\n",
      "epoch[2/5], MiniBatch-209 loss:0.0159\n",
      "epoch[2/5], MiniBatch-210 loss:0.0156\n",
      "epoch[2/5], MiniBatch-211 loss:0.0154\n",
      "epoch[2/5], MiniBatch-212 loss:0.0158\n",
      "epoch[2/5], MiniBatch-213 loss:0.0156\n",
      "epoch[2/5], MiniBatch-214 loss:0.0158\n",
      "epoch[2/5], MiniBatch-215 loss:0.0158\n",
      "epoch[2/5], MiniBatch-216 loss:0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2/5], MiniBatch-217 loss:0.0159\n",
      "epoch[2/5], MiniBatch-218 loss:0.0156\n",
      "epoch[2/5], MiniBatch-219 loss:0.0155\n",
      "epoch[2/5], MiniBatch-220 loss:0.0159\n",
      "epoch[2/5], MiniBatch-221 loss:0.0157\n",
      "epoch[2/5], MiniBatch-222 loss:0.0160\n",
      "epoch[2/5], MiniBatch-223 loss:0.0153\n",
      "epoch[2/5], MiniBatch-224 loss:0.0157\n",
      "epoch[2/5], MiniBatch-225 loss:0.0158\n",
      "epoch[2/5], MiniBatch-226 loss:0.0158\n",
      "epoch[2/5], MiniBatch-227 loss:0.0155\n",
      "epoch[2/5], MiniBatch-228 loss:0.0157\n",
      "epoch[2/5], MiniBatch-229 loss:0.0161\n",
      "epoch[2/5], MiniBatch-230 loss:0.0157\n",
      "epoch[2/5], MiniBatch-231 loss:0.0155\n",
      "epoch[2/5], MiniBatch-232 loss:0.0155\n",
      "epoch[2/5], MiniBatch-233 loss:0.0163\n",
      "epoch[2/5], MiniBatch-234 loss:0.0160\n",
      "epoch[2/5], MiniBatch-235 loss:0.0161\n",
      "epoch[2/5], MiniBatch-236 loss:0.0160\n",
      "epoch[2/5], MiniBatch-237 loss:0.0158\n",
      "epoch[2/5], MiniBatch-238 loss:0.0156\n",
      "epoch[2/5], MiniBatch-239 loss:0.0157\n",
      "epoch[2/5], MiniBatch-240 loss:0.0157\n",
      "epoch[2/5], MiniBatch-241 loss:0.0156\n",
      "epoch[2/5], MiniBatch-242 loss:0.0155\n",
      "epoch[2/5], MiniBatch-243 loss:0.0154\n",
      "epoch[2/5], MiniBatch-244 loss:0.0150\n",
      "epoch[2/5], MiniBatch-245 loss:0.0158\n",
      "epoch[2/5], MiniBatch-246 loss:0.0158\n",
      "epoch[2/5], MiniBatch-247 loss:0.0157\n",
      "epoch[2/5], MiniBatch-248 loss:0.0155\n",
      "epoch[2/5], MiniBatch-249 loss:0.0158\n",
      "epoch[2/5], MiniBatch-250 loss:0.0162\n",
      "epoch[2/5], MiniBatch-251 loss:0.0156\n",
      "epoch[2/5], MiniBatch-252 loss:0.0158\n",
      "epoch[2/5], MiniBatch-253 loss:0.0155\n",
      "epoch[2/5], MiniBatch-254 loss:0.0157\n",
      "epoch[2/5], MiniBatch-255 loss:0.0162\n",
      "epoch[2/5], MiniBatch-256 loss:0.0159\n",
      "epoch[2/5], MiniBatch-257 loss:0.0158\n",
      "epoch[2/5], MiniBatch-258 loss:0.0157\n",
      "epoch[2/5], MiniBatch-259 loss:0.0153\n",
      "epoch[2/5], MiniBatch-260 loss:0.0155\n",
      "epoch[2/5], MiniBatch-261 loss:0.0155\n",
      "epoch[2/5], MiniBatch-262 loss:0.0156\n",
      "epoch[2/5], MiniBatch-263 loss:0.0154\n",
      "epoch[2/5], MiniBatch-264 loss:0.0156\n",
      "epoch[2/5], MiniBatch-265 loss:0.0155\n",
      "epoch[2/5], MiniBatch-266 loss:0.0163\n",
      "epoch[2/5], MiniBatch-267 loss:0.0159\n",
      "epoch[2/5], MiniBatch-268 loss:0.0157\n",
      "epoch[2/5], MiniBatch-269 loss:0.0159\n",
      "epoch[2/5], MiniBatch-270 loss:0.0157\n",
      "epoch[2/5], MiniBatch-271 loss:0.0155\n",
      "epoch[2/5], MiniBatch-272 loss:0.0159\n",
      "epoch[2/5], MiniBatch-273 loss:0.0159\n",
      "epoch[2/5], MiniBatch-274 loss:0.0156\n",
      "epoch[2/5], MiniBatch-275 loss:0.0162\n",
      "epoch[2/5], MiniBatch-276 loss:0.0159\n",
      "epoch[2/5], MiniBatch-277 loss:0.0156\n",
      "epoch[2/5], MiniBatch-278 loss:0.0157\n",
      "epoch[2/5], MiniBatch-279 loss:0.0155\n",
      "epoch[2/5], MiniBatch-280 loss:0.0159\n",
      "epoch[2/5], MiniBatch-281 loss:0.0156\n",
      "epoch[2/5], MiniBatch-282 loss:0.0159\n",
      "epoch[2/5], MiniBatch-283 loss:0.0158\n",
      "epoch[2/5], MiniBatch-284 loss:0.0154\n",
      "epoch[2/5], MiniBatch-285 loss:0.0158\n",
      "epoch[2/5], MiniBatch-286 loss:0.0160\n",
      "epoch[2/5], MiniBatch-287 loss:0.0159\n",
      "epoch[2/5], MiniBatch-288 loss:0.0158\n",
      "epoch[2/5], MiniBatch-289 loss:0.0160\n",
      "epoch[2/5], MiniBatch-290 loss:0.0159\n",
      "epoch[2/5], MiniBatch-291 loss:0.0159\n",
      "epoch[2/5], MiniBatch-292 loss:0.0152\n",
      "epoch[2/5], MiniBatch-293 loss:0.0160\n",
      "epoch[2/5], MiniBatch-294 loss:0.0161\n",
      "epoch[2/5], MiniBatch-295 loss:0.0160\n",
      "epoch[2/5], MiniBatch-296 loss:0.0163\n",
      "epoch[2/5], MiniBatch-297 loss:0.0156\n",
      "epoch[2/5], MiniBatch-298 loss:0.0155\n",
      "epoch[2/5], MiniBatch-299 loss:0.0156\n",
      "epoch[2/5], MiniBatch-300 loss:0.0163\n",
      "epoch[2/5], MiniBatch-301 loss:0.0161\n",
      "epoch[2/5], MiniBatch-302 loss:0.0155\n",
      "epoch[2/5], MiniBatch-303 loss:0.0157\n",
      "epoch[2/5], MiniBatch-304 loss:0.0159\n",
      "epoch[2/5], MiniBatch-305 loss:0.0159\n",
      "epoch[2/5], MiniBatch-306 loss:0.0162\n",
      "epoch[2/5], MiniBatch-307 loss:0.0155\n",
      "epoch[2/5], MiniBatch-308 loss:0.0153\n",
      "epoch[2/5], MiniBatch-309 loss:0.0156\n",
      "epoch[2/5], MiniBatch-310 loss:0.0158\n",
      "epoch[2/5], MiniBatch-311 loss:0.0154\n",
      "epoch[2/5], MiniBatch-312 loss:0.0160\n",
      "epoch[2/5], MiniBatch-313 loss:0.0156\n",
      "epoch[2/5], MiniBatch-314 loss:0.0157\n",
      "epoch[2/5], MiniBatch-315 loss:0.0157\n",
      "epoch[2/5], MiniBatch-316 loss:0.0154\n",
      "epoch[2/5], MiniBatch-317 loss:0.0160\n",
      "epoch[2/5], MiniBatch-318 loss:0.0158\n",
      "epoch[2/5], MiniBatch-319 loss:0.0159\n",
      "epoch[2/5], MiniBatch-320 loss:0.0158\n",
      "epoch[2/5], MiniBatch-321 loss:0.0161\n",
      "epoch[2/5], MiniBatch-322 loss:0.0159\n",
      "epoch[2/5], MiniBatch-323 loss:0.0155\n",
      "epoch[2/5], MiniBatch-324 loss:0.0157\n",
      "epoch[2/5], MiniBatch-325 loss:0.0157\n",
      "epoch[2/5], MiniBatch-326 loss:0.0162\n",
      "epoch[2/5], MiniBatch-327 loss:0.0157\n",
      "epoch[2/5], MiniBatch-328 loss:0.0156\n",
      "epoch[2/5], MiniBatch-329 loss:0.0156\n",
      "epoch[2/5], MiniBatch-330 loss:0.0156\n",
      "epoch[2/5], MiniBatch-331 loss:0.0162\n",
      "epoch[2/5], MiniBatch-332 loss:0.0158\n",
      "epoch[2/5], MiniBatch-333 loss:0.0158\n",
      "epoch[2/5], MiniBatch-334 loss:0.0157\n",
      "epoch[2/5], MiniBatch-335 loss:0.0158\n",
      "epoch[2/5], MiniBatch-336 loss:0.0160\n",
      "epoch[2/5], MiniBatch-337 loss:0.0156\n",
      "epoch[2/5], MiniBatch-338 loss:0.0156\n",
      "epoch[2/5], MiniBatch-339 loss:0.0155\n",
      "epoch[2/5], MiniBatch-340 loss:0.0158\n",
      "epoch[2/5], MiniBatch-341 loss:0.0156\n",
      "epoch[2/5], MiniBatch-342 loss:0.0157\n",
      "epoch[2/5], MiniBatch-343 loss:0.0157\n",
      "epoch[2/5], MiniBatch-344 loss:0.0158\n",
      "epoch[2/5], MiniBatch-345 loss:0.0160\n",
      "epoch[2/5], MiniBatch-346 loss:0.0160\n",
      "epoch[2/5], MiniBatch-347 loss:0.0156\n",
      "epoch[2/5], MiniBatch-348 loss:0.0161\n",
      "epoch[2/5], MiniBatch-349 loss:0.0155\n",
      "epoch[2/5], MiniBatch-350 loss:0.0156\n",
      "epoch[2/5], MiniBatch-351 loss:0.0157\n",
      "epoch[2/5], MiniBatch-352 loss:0.0156\n",
      "epoch[2/5], MiniBatch-353 loss:0.0158\n",
      "epoch[2/5], MiniBatch-354 loss:0.0156\n",
      "epoch[2/5], MiniBatch-355 loss:0.0156\n",
      "epoch[2/5], MiniBatch-356 loss:0.0162\n",
      "epoch[2/5], MiniBatch-357 loss:0.0156\n",
      "epoch[2/5], MiniBatch-358 loss:0.0155\n",
      "epoch[2/5], MiniBatch-359 loss:0.0154\n",
      "epoch[2/5], MiniBatch-360 loss:0.0156\n",
      "epoch[2/5], MiniBatch-361 loss:0.0159\n",
      "epoch[2/5], MiniBatch-362 loss:0.0160\n",
      "epoch[2/5], MiniBatch-363 loss:0.0160\n",
      "epoch[2/5], MiniBatch-364 loss:0.0160\n",
      "epoch[2/5], MiniBatch-365 loss:0.0158\n",
      "epoch[2/5], MiniBatch-366 loss:0.0158\n",
      "epoch[2/5], MiniBatch-367 loss:0.0159\n",
      "epoch[2/5], MiniBatch-368 loss:0.0159\n",
      "epoch[2/5], MiniBatch-369 loss:0.0159\n",
      "epoch[2/5], MiniBatch-370 loss:0.0158\n",
      "epoch[2/5], MiniBatch-371 loss:0.0161\n",
      "epoch[2/5], MiniBatch-372 loss:0.0155\n",
      "epoch[2/5], MiniBatch-373 loss:0.0160\n",
      "epoch[2/5], MiniBatch-374 loss:0.0159\n",
      "epoch[2/5], MiniBatch-375 loss:0.0156\n",
      "epoch[2/5], MiniBatch-376 loss:0.0157\n",
      "epoch[2/5], MiniBatch-377 loss:0.0161\n",
      "epoch[2/5], MiniBatch-378 loss:0.0156\n",
      "epoch[2/5], MiniBatch-379 loss:0.0157\n",
      "epoch[2/5], MiniBatch-380 loss:0.0159\n",
      "epoch[2/5], MiniBatch-381 loss:0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:43<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2/5], [-----TEST------] loss:0.0322  Accur:0.9331\n",
      "***saving best optimal state [Loss:0.032230038195848465 Accur:0.9331367611885071] ***\n",
      "epoch[3/5], MiniBatch-1 loss:0.0152\n",
      "epoch[3/5], MiniBatch-2 loss:0.0153\n",
      "epoch[3/5], MiniBatch-3 loss:0.0150\n",
      "epoch[3/5], MiniBatch-4 loss:0.0154\n",
      "epoch[3/5], MiniBatch-5 loss:0.0151\n",
      "epoch[3/5], MiniBatch-6 loss:0.0151\n",
      "epoch[3/5], MiniBatch-7 loss:0.0152\n",
      "epoch[3/5], MiniBatch-8 loss:0.0157\n",
      "epoch[3/5], MiniBatch-9 loss:0.0151\n",
      "epoch[3/5], MiniBatch-10 loss:0.0154\n",
      "epoch[3/5], MiniBatch-11 loss:0.0157\n",
      "epoch[3/5], MiniBatch-12 loss:0.0153\n",
      "epoch[3/5], MiniBatch-13 loss:0.0154\n",
      "epoch[3/5], MiniBatch-14 loss:0.0156\n",
      "epoch[3/5], MiniBatch-15 loss:0.0154\n",
      "epoch[3/5], MiniBatch-16 loss:0.0151\n",
      "epoch[3/5], MiniBatch-17 loss:0.0153\n",
      "epoch[3/5], MiniBatch-18 loss:0.0153\n",
      "epoch[3/5], MiniBatch-19 loss:0.0155\n",
      "epoch[3/5], MiniBatch-20 loss:0.0157\n",
      "epoch[3/5], MiniBatch-21 loss:0.0152\n",
      "epoch[3/5], MiniBatch-22 loss:0.0155\n",
      "epoch[3/5], MiniBatch-23 loss:0.0153\n",
      "epoch[3/5], MiniBatch-24 loss:0.0157\n",
      "epoch[3/5], MiniBatch-25 loss:0.0155\n",
      "epoch[3/5], MiniBatch-26 loss:0.0150\n",
      "epoch[3/5], MiniBatch-27 loss:0.0152\n",
      "epoch[3/5], MiniBatch-28 loss:0.0149\n",
      "epoch[3/5], MiniBatch-29 loss:0.0153\n",
      "epoch[3/5], MiniBatch-30 loss:0.0151\n",
      "epoch[3/5], MiniBatch-31 loss:0.0150\n",
      "epoch[3/5], MiniBatch-32 loss:0.0153\n",
      "epoch[3/5], MiniBatch-33 loss:0.0152\n",
      "epoch[3/5], MiniBatch-34 loss:0.0152\n",
      "epoch[3/5], MiniBatch-35 loss:0.0154\n",
      "epoch[3/5], MiniBatch-36 loss:0.0155\n",
      "epoch[3/5], MiniBatch-37 loss:0.0154\n",
      "epoch[3/5], MiniBatch-38 loss:0.0152\n",
      "epoch[3/5], MiniBatch-39 loss:0.0154\n",
      "epoch[3/5], MiniBatch-40 loss:0.0158\n",
      "epoch[3/5], MiniBatch-41 loss:0.0152\n",
      "epoch[3/5], MiniBatch-42 loss:0.0152\n",
      "epoch[3/5], MiniBatch-43 loss:0.0154\n",
      "epoch[3/5], MiniBatch-44 loss:0.0152\n",
      "epoch[3/5], MiniBatch-45 loss:0.0151\n",
      "epoch[3/5], MiniBatch-46 loss:0.0150\n",
      "epoch[3/5], MiniBatch-47 loss:0.0150\n",
      "epoch[3/5], MiniBatch-48 loss:0.0152\n",
      "epoch[3/5], MiniBatch-49 loss:0.0155\n",
      "epoch[3/5], MiniBatch-50 loss:0.0151\n",
      "epoch[3/5], MiniBatch-51 loss:0.0151\n",
      "epoch[3/5], MiniBatch-52 loss:0.0151\n",
      "epoch[3/5], MiniBatch-53 loss:0.0154\n",
      "epoch[3/5], MiniBatch-54 loss:0.0154\n",
      "epoch[3/5], MiniBatch-55 loss:0.0154\n",
      "epoch[3/5], MiniBatch-56 loss:0.0150\n",
      "epoch[3/5], MiniBatch-57 loss:0.0156\n",
      "epoch[3/5], MiniBatch-58 loss:0.0152\n",
      "epoch[3/5], MiniBatch-59 loss:0.0154\n",
      "epoch[3/5], MiniBatch-60 loss:0.0155\n",
      "epoch[3/5], MiniBatch-61 loss:0.0152\n",
      "epoch[3/5], MiniBatch-62 loss:0.0154\n",
      "epoch[3/5], MiniBatch-63 loss:0.0153\n",
      "epoch[3/5], MiniBatch-64 loss:0.0151\n",
      "epoch[3/5], MiniBatch-65 loss:0.0153\n",
      "epoch[3/5], MiniBatch-66 loss:0.0154\n",
      "epoch[3/5], MiniBatch-67 loss:0.0155\n",
      "epoch[3/5], MiniBatch-68 loss:0.0154\n",
      "epoch[3/5], MiniBatch-69 loss:0.0151\n",
      "epoch[3/5], MiniBatch-70 loss:0.0164\n",
      "epoch[3/5], MiniBatch-71 loss:0.0153\n",
      "epoch[3/5], MiniBatch-72 loss:0.0154\n",
      "epoch[3/5], MiniBatch-73 loss:0.0156\n",
      "epoch[3/5], MiniBatch-74 loss:0.0155\n",
      "epoch[3/5], MiniBatch-75 loss:0.0153\n",
      "epoch[3/5], MiniBatch-76 loss:0.0154\n",
      "epoch[3/5], MiniBatch-77 loss:0.0154\n",
      "epoch[3/5], MiniBatch-78 loss:0.0154\n",
      "epoch[3/5], MiniBatch-79 loss:0.0154\n",
      "epoch[3/5], MiniBatch-80 loss:0.0155\n",
      "epoch[3/5], MiniBatch-81 loss:0.0157\n",
      "epoch[3/5], MiniBatch-82 loss:0.0154\n",
      "epoch[3/5], MiniBatch-83 loss:0.0153\n",
      "epoch[3/5], MiniBatch-84 loss:0.0155\n",
      "epoch[3/5], MiniBatch-85 loss:0.0153\n",
      "epoch[3/5], MiniBatch-86 loss:0.0155\n",
      "epoch[3/5], MiniBatch-87 loss:0.0154\n",
      "epoch[3/5], MiniBatch-88 loss:0.0153\n",
      "epoch[3/5], MiniBatch-89 loss:0.0154\n",
      "epoch[3/5], MiniBatch-90 loss:0.0153\n",
      "epoch[3/5], MiniBatch-91 loss:0.0149\n",
      "epoch[3/5], MiniBatch-92 loss:0.0157\n",
      "epoch[3/5], MiniBatch-93 loss:0.0156\n",
      "epoch[3/5], MiniBatch-94 loss:0.0150\n",
      "epoch[3/5], MiniBatch-95 loss:0.0154\n",
      "epoch[3/5], MiniBatch-96 loss:0.0154\n",
      "epoch[3/5], MiniBatch-97 loss:0.0151\n",
      "epoch[3/5], MiniBatch-98 loss:0.0154\n",
      "epoch[3/5], MiniBatch-99 loss:0.0154\n",
      "epoch[3/5], MiniBatch-100 loss:0.0157\n",
      "epoch[3/5], MiniBatch-101 loss:0.0158\n",
      "epoch[3/5], MiniBatch-102 loss:0.0153\n",
      "epoch[3/5], MiniBatch-103 loss:0.0151\n",
      "epoch[3/5], MiniBatch-104 loss:0.0156\n",
      "epoch[3/5], MiniBatch-105 loss:0.0152\n",
      "epoch[3/5], MiniBatch-106 loss:0.0155\n",
      "epoch[3/5], MiniBatch-107 loss:0.0153\n",
      "epoch[3/5], MiniBatch-108 loss:0.0155\n",
      "epoch[3/5], MiniBatch-109 loss:0.0155\n",
      "epoch[3/5], MiniBatch-110 loss:0.0152\n",
      "epoch[3/5], MiniBatch-111 loss:0.0154\n",
      "epoch[3/5], MiniBatch-112 loss:0.0155\n",
      "epoch[3/5], MiniBatch-113 loss:0.0156\n",
      "epoch[3/5], MiniBatch-114 loss:0.0155\n",
      "epoch[3/5], MiniBatch-115 loss:0.0155\n",
      "epoch[3/5], MiniBatch-116 loss:0.0155\n",
      "epoch[3/5], MiniBatch-117 loss:0.0153\n",
      "epoch[3/5], MiniBatch-118 loss:0.0155\n",
      "epoch[3/5], MiniBatch-119 loss:0.0150\n",
      "epoch[3/5], MiniBatch-120 loss:0.0160\n",
      "epoch[3/5], MiniBatch-121 loss:0.0152\n",
      "epoch[3/5], MiniBatch-122 loss:0.0155\n",
      "epoch[3/5], MiniBatch-123 loss:0.0154\n",
      "epoch[3/5], MiniBatch-124 loss:0.0154\n",
      "epoch[3/5], MiniBatch-125 loss:0.0158\n",
      "epoch[3/5], MiniBatch-126 loss:0.0153\n",
      "epoch[3/5], MiniBatch-127 loss:0.0151\n",
      "epoch[3/5], MiniBatch-128 loss:0.0152\n",
      "epoch[3/5], MiniBatch-129 loss:0.0153\n",
      "epoch[3/5], MiniBatch-130 loss:0.0155\n",
      "epoch[3/5], MiniBatch-131 loss:0.0155\n",
      "epoch[3/5], MiniBatch-132 loss:0.0153\n",
      "epoch[3/5], MiniBatch-133 loss:0.0153\n",
      "epoch[3/5], MiniBatch-134 loss:0.0151\n",
      "epoch[3/5], MiniBatch-135 loss:0.0157\n",
      "epoch[3/5], MiniBatch-136 loss:0.0155\n",
      "epoch[3/5], MiniBatch-137 loss:0.0149\n",
      "epoch[3/5], MiniBatch-138 loss:0.0155\n",
      "epoch[3/5], MiniBatch-139 loss:0.0154\n",
      "epoch[3/5], MiniBatch-140 loss:0.0158\n",
      "epoch[3/5], MiniBatch-141 loss:0.0151\n",
      "epoch[3/5], MiniBatch-142 loss:0.0155\n",
      "epoch[3/5], MiniBatch-143 loss:0.0157\n",
      "epoch[3/5], MiniBatch-144 loss:0.0153\n",
      "epoch[3/5], MiniBatch-145 loss:0.0152\n",
      "epoch[3/5], MiniBatch-146 loss:0.0160\n",
      "epoch[3/5], MiniBatch-147 loss:0.0152\n",
      "epoch[3/5], MiniBatch-148 loss:0.0157\n",
      "epoch[3/5], MiniBatch-149 loss:0.0153\n",
      "epoch[3/5], MiniBatch-150 loss:0.0156\n",
      "epoch[3/5], MiniBatch-151 loss:0.0154\n",
      "epoch[3/5], MiniBatch-152 loss:0.0156\n",
      "epoch[3/5], MiniBatch-153 loss:0.0154\n",
      "epoch[3/5], MiniBatch-154 loss:0.0154\n",
      "epoch[3/5], MiniBatch-155 loss:0.0154\n",
      "epoch[3/5], MiniBatch-156 loss:0.0153\n",
      "epoch[3/5], MiniBatch-157 loss:0.0156\n",
      "epoch[3/5], MiniBatch-158 loss:0.0157\n",
      "epoch[3/5], MiniBatch-159 loss:0.0157\n",
      "epoch[3/5], MiniBatch-160 loss:0.0156\n",
      "epoch[3/5], MiniBatch-161 loss:0.0158\n",
      "epoch[3/5], MiniBatch-162 loss:0.0159\n",
      "epoch[3/5], MiniBatch-163 loss:0.0157\n",
      "epoch[3/5], MiniBatch-164 loss:0.0153\n",
      "epoch[3/5], MiniBatch-165 loss:0.0154\n",
      "epoch[3/5], MiniBatch-166 loss:0.0153\n",
      "epoch[3/5], MiniBatch-167 loss:0.0158\n",
      "epoch[3/5], MiniBatch-168 loss:0.0155\n",
      "epoch[3/5], MiniBatch-169 loss:0.0151\n",
      "epoch[3/5], MiniBatch-170 loss:0.0158\n",
      "epoch[3/5], MiniBatch-171 loss:0.0154\n",
      "epoch[3/5], MiniBatch-172 loss:0.0159\n",
      "epoch[3/5], MiniBatch-173 loss:0.0153\n",
      "epoch[3/5], MiniBatch-174 loss:0.0151\n",
      "epoch[3/5], MiniBatch-175 loss:0.0150\n",
      "epoch[3/5], MiniBatch-176 loss:0.0156\n",
      "epoch[3/5], MiniBatch-177 loss:0.0154\n",
      "epoch[3/5], MiniBatch-178 loss:0.0154\n",
      "epoch[3/5], MiniBatch-179 loss:0.0157\n",
      "epoch[3/5], MiniBatch-180 loss:0.0159\n",
      "epoch[3/5], MiniBatch-181 loss:0.0156\n",
      "epoch[3/5], MiniBatch-182 loss:0.0155\n",
      "epoch[3/5], MiniBatch-183 loss:0.0153\n",
      "epoch[3/5], MiniBatch-184 loss:0.0160\n",
      "epoch[3/5], MiniBatch-185 loss:0.0155\n",
      "epoch[3/5], MiniBatch-186 loss:0.0156\n",
      "epoch[3/5], MiniBatch-187 loss:0.0156\n",
      "epoch[3/5], MiniBatch-188 loss:0.0159\n",
      "epoch[3/5], MiniBatch-189 loss:0.0152\n",
      "epoch[3/5], MiniBatch-190 loss:0.0153\n",
      "epoch[3/5], MiniBatch-191 loss:0.0158\n",
      "epoch[3/5], MiniBatch-192 loss:0.0157\n",
      "epoch[3/5], MiniBatch-193 loss:0.0159\n",
      "epoch[3/5], MiniBatch-194 loss:0.0151\n",
      "epoch[3/5], MiniBatch-195 loss:0.0154\n",
      "epoch[3/5], MiniBatch-196 loss:0.0159\n",
      "epoch[3/5], MiniBatch-197 loss:0.0158\n",
      "epoch[3/5], MiniBatch-198 loss:0.0158\n",
      "epoch[3/5], MiniBatch-199 loss:0.0154\n",
      "epoch[3/5], MiniBatch-200 loss:0.0156\n",
      "epoch[3/5], MiniBatch-201 loss:0.0156\n",
      "epoch[3/5], MiniBatch-202 loss:0.0156\n",
      "epoch[3/5], MiniBatch-203 loss:0.0156\n",
      "epoch[3/5], MiniBatch-204 loss:0.0159\n",
      "epoch[3/5], MiniBatch-205 loss:0.0155\n",
      "epoch[3/5], MiniBatch-206 loss:0.0157\n",
      "epoch[3/5], MiniBatch-207 loss:0.0158\n",
      "epoch[3/5], MiniBatch-208 loss:0.0156\n",
      "epoch[3/5], MiniBatch-209 loss:0.0159\n",
      "epoch[3/5], MiniBatch-210 loss:0.0151\n",
      "epoch[3/5], MiniBatch-211 loss:0.0154\n",
      "epoch[3/5], MiniBatch-212 loss:0.0158\n",
      "epoch[3/5], MiniBatch-213 loss:0.0159\n",
      "epoch[3/5], MiniBatch-214 loss:0.0155\n",
      "epoch[3/5], MiniBatch-215 loss:0.0154\n",
      "epoch[3/5], MiniBatch-216 loss:0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3/5], MiniBatch-217 loss:0.0155\n",
      "epoch[3/5], MiniBatch-218 loss:0.0153\n",
      "epoch[3/5], MiniBatch-219 loss:0.0150\n",
      "epoch[3/5], MiniBatch-220 loss:0.0155\n",
      "epoch[3/5], MiniBatch-221 loss:0.0153\n",
      "epoch[3/5], MiniBatch-222 loss:0.0152\n",
      "epoch[3/5], MiniBatch-223 loss:0.0159\n",
      "epoch[3/5], MiniBatch-224 loss:0.0163\n",
      "epoch[3/5], MiniBatch-225 loss:0.0159\n",
      "epoch[3/5], MiniBatch-226 loss:0.0155\n",
      "epoch[3/5], MiniBatch-227 loss:0.0161\n",
      "epoch[3/5], MiniBatch-228 loss:0.0154\n",
      "epoch[3/5], MiniBatch-229 loss:0.0157\n",
      "epoch[3/5], MiniBatch-230 loss:0.0157\n",
      "epoch[3/5], MiniBatch-231 loss:0.0156\n",
      "epoch[3/5], MiniBatch-232 loss:0.0155\n",
      "epoch[3/5], MiniBatch-233 loss:0.0154\n",
      "epoch[3/5], MiniBatch-234 loss:0.0158\n",
      "epoch[3/5], MiniBatch-235 loss:0.0159\n",
      "epoch[3/5], MiniBatch-236 loss:0.0158\n",
      "epoch[3/5], MiniBatch-237 loss:0.0160\n",
      "epoch[3/5], MiniBatch-238 loss:0.0159\n",
      "epoch[3/5], MiniBatch-239 loss:0.0158\n",
      "epoch[3/5], MiniBatch-240 loss:0.0157\n",
      "epoch[3/5], MiniBatch-241 loss:0.0160\n",
      "epoch[3/5], MiniBatch-242 loss:0.0158\n",
      "epoch[3/5], MiniBatch-243 loss:0.0158\n",
      "epoch[3/5], MiniBatch-244 loss:0.0155\n",
      "epoch[3/5], MiniBatch-245 loss:0.0156\n",
      "epoch[3/5], MiniBatch-246 loss:0.0158\n",
      "epoch[3/5], MiniBatch-247 loss:0.0159\n",
      "epoch[3/5], MiniBatch-248 loss:0.0153\n",
      "epoch[3/5], MiniBatch-249 loss:0.0156\n",
      "epoch[3/5], MiniBatch-250 loss:0.0162\n",
      "epoch[3/5], MiniBatch-251 loss:0.0157\n",
      "epoch[3/5], MiniBatch-252 loss:0.0160\n",
      "epoch[3/5], MiniBatch-253 loss:0.0158\n",
      "epoch[3/5], MiniBatch-254 loss:0.0159\n",
      "epoch[3/5], MiniBatch-255 loss:0.0155\n",
      "epoch[3/5], MiniBatch-256 loss:0.0157\n",
      "epoch[3/5], MiniBatch-257 loss:0.0156\n",
      "epoch[3/5], MiniBatch-258 loss:0.0156\n",
      "epoch[3/5], MiniBatch-259 loss:0.0160\n",
      "epoch[3/5], MiniBatch-260 loss:0.0153\n",
      "epoch[3/5], MiniBatch-261 loss:0.0157\n",
      "epoch[3/5], MiniBatch-262 loss:0.0156\n",
      "epoch[3/5], MiniBatch-263 loss:0.0160\n",
      "epoch[3/5], MiniBatch-264 loss:0.0160\n",
      "epoch[3/5], MiniBatch-265 loss:0.0157\n",
      "epoch[3/5], MiniBatch-266 loss:0.0154\n",
      "epoch[3/5], MiniBatch-267 loss:0.0156\n",
      "epoch[3/5], MiniBatch-268 loss:0.0159\n",
      "epoch[3/5], MiniBatch-269 loss:0.0156\n",
      "epoch[3/5], MiniBatch-270 loss:0.0157\n",
      "epoch[3/5], MiniBatch-271 loss:0.0159\n",
      "epoch[3/5], MiniBatch-272 loss:0.0161\n",
      "epoch[3/5], MiniBatch-273 loss:0.0156\n",
      "epoch[3/5], MiniBatch-274 loss:0.0155\n",
      "epoch[3/5], MiniBatch-275 loss:0.0156\n",
      "epoch[3/5], MiniBatch-276 loss:0.0159\n",
      "epoch[3/5], MiniBatch-277 loss:0.0155\n",
      "epoch[3/5], MiniBatch-278 loss:0.0158\n",
      "epoch[3/5], MiniBatch-279 loss:0.0158\n",
      "epoch[3/5], MiniBatch-280 loss:0.0153\n",
      "epoch[3/5], MiniBatch-281 loss:0.0156\n",
      "epoch[3/5], MiniBatch-282 loss:0.0161\n",
      "epoch[3/5], MiniBatch-283 loss:0.0154\n",
      "epoch[3/5], MiniBatch-284 loss:0.0157\n",
      "epoch[3/5], MiniBatch-285 loss:0.0159\n",
      "epoch[3/5], MiniBatch-286 loss:0.0156\n",
      "epoch[3/5], MiniBatch-287 loss:0.0156\n",
      "epoch[3/5], MiniBatch-288 loss:0.0158\n",
      "epoch[3/5], MiniBatch-289 loss:0.0161\n",
      "epoch[3/5], MiniBatch-290 loss:0.0158\n",
      "epoch[3/5], MiniBatch-291 loss:0.0151\n",
      "epoch[3/5], MiniBatch-292 loss:0.0156\n",
      "epoch[3/5], MiniBatch-293 loss:0.0158\n",
      "epoch[3/5], MiniBatch-294 loss:0.0155\n",
      "epoch[3/5], MiniBatch-295 loss:0.0158\n",
      "epoch[3/5], MiniBatch-296 loss:0.0157\n",
      "epoch[3/5], MiniBatch-297 loss:0.0157\n",
      "epoch[3/5], MiniBatch-298 loss:0.0154\n",
      "epoch[3/5], MiniBatch-299 loss:0.0158\n",
      "epoch[3/5], MiniBatch-300 loss:0.0160\n",
      "epoch[3/5], MiniBatch-301 loss:0.0153\n",
      "epoch[3/5], MiniBatch-302 loss:0.0158\n",
      "epoch[3/5], MiniBatch-303 loss:0.0159\n",
      "epoch[3/5], MiniBatch-304 loss:0.0159\n",
      "epoch[3/5], MiniBatch-305 loss:0.0154\n",
      "epoch[3/5], MiniBatch-306 loss:0.0155\n",
      "epoch[3/5], MiniBatch-307 loss:0.0157\n",
      "epoch[3/5], MiniBatch-308 loss:0.0161\n",
      "epoch[3/5], MiniBatch-309 loss:0.0156\n",
      "epoch[3/5], MiniBatch-310 loss:0.0159\n",
      "epoch[3/5], MiniBatch-311 loss:0.0159\n",
      "epoch[3/5], MiniBatch-312 loss:0.0159\n",
      "epoch[3/5], MiniBatch-313 loss:0.0158\n",
      "epoch[3/5], MiniBatch-314 loss:0.0158\n",
      "epoch[3/5], MiniBatch-315 loss:0.0156\n",
      "epoch[3/5], MiniBatch-316 loss:0.0156\n",
      "epoch[3/5], MiniBatch-317 loss:0.0151\n",
      "epoch[3/5], MiniBatch-318 loss:0.0160\n",
      "epoch[3/5], MiniBatch-319 loss:0.0155\n",
      "epoch[3/5], MiniBatch-320 loss:0.0157\n",
      "epoch[3/5], MiniBatch-321 loss:0.0158\n",
      "epoch[3/5], MiniBatch-322 loss:0.0154\n",
      "epoch[3/5], MiniBatch-323 loss:0.0160\n",
      "epoch[3/5], MiniBatch-324 loss:0.0156\n",
      "epoch[3/5], MiniBatch-325 loss:0.0155\n",
      "epoch[3/5], MiniBatch-326 loss:0.0162\n",
      "epoch[3/5], MiniBatch-327 loss:0.0163\n",
      "epoch[3/5], MiniBatch-328 loss:0.0155\n",
      "epoch[3/5], MiniBatch-329 loss:0.0159\n",
      "epoch[3/5], MiniBatch-330 loss:0.0162\n",
      "epoch[3/5], MiniBatch-331 loss:0.0159\n",
      "epoch[3/5], MiniBatch-332 loss:0.0158\n",
      "epoch[3/5], MiniBatch-333 loss:0.0161\n",
      "epoch[3/5], MiniBatch-334 loss:0.0156\n",
      "epoch[3/5], MiniBatch-335 loss:0.0156\n",
      "epoch[3/5], MiniBatch-336 loss:0.0158\n",
      "epoch[3/5], MiniBatch-337 loss:0.0160\n",
      "epoch[3/5], MiniBatch-338 loss:0.0159\n",
      "epoch[3/5], MiniBatch-339 loss:0.0156\n",
      "epoch[3/5], MiniBatch-340 loss:0.0158\n",
      "epoch[3/5], MiniBatch-341 loss:0.0153\n",
      "epoch[3/5], MiniBatch-342 loss:0.0160\n",
      "epoch[3/5], MiniBatch-343 loss:0.0157\n",
      "epoch[3/5], MiniBatch-344 loss:0.0157\n",
      "epoch[3/5], MiniBatch-345 loss:0.0158\n",
      "epoch[3/5], MiniBatch-346 loss:0.0157\n",
      "epoch[3/5], MiniBatch-347 loss:0.0160\n",
      "epoch[3/5], MiniBatch-348 loss:0.0159\n",
      "epoch[3/5], MiniBatch-349 loss:0.0162\n",
      "epoch[3/5], MiniBatch-350 loss:0.0155\n",
      "epoch[3/5], MiniBatch-351 loss:0.0158\n",
      "epoch[3/5], MiniBatch-352 loss:0.0158\n",
      "epoch[3/5], MiniBatch-353 loss:0.0162\n",
      "epoch[3/5], MiniBatch-354 loss:0.0160\n",
      "epoch[3/5], MiniBatch-355 loss:0.0158\n",
      "epoch[3/5], MiniBatch-356 loss:0.0162\n",
      "epoch[3/5], MiniBatch-357 loss:0.0157\n",
      "epoch[3/5], MiniBatch-358 loss:0.0163\n",
      "epoch[3/5], MiniBatch-359 loss:0.0159\n",
      "epoch[3/5], MiniBatch-360 loss:0.0156\n",
      "epoch[3/5], MiniBatch-361 loss:0.0156\n",
      "epoch[3/5], MiniBatch-362 loss:0.0161\n",
      "epoch[3/5], MiniBatch-363 loss:0.0156\n",
      "epoch[3/5], MiniBatch-364 loss:0.0158\n",
      "epoch[3/5], MiniBatch-365 loss:0.0160\n",
      "epoch[3/5], MiniBatch-366 loss:0.0161\n",
      "epoch[3/5], MiniBatch-367 loss:0.0158\n",
      "epoch[3/5], MiniBatch-368 loss:0.0160\n",
      "epoch[3/5], MiniBatch-369 loss:0.0158\n",
      "epoch[3/5], MiniBatch-370 loss:0.0158\n",
      "epoch[3/5], MiniBatch-371 loss:0.0166\n",
      "epoch[3/5], MiniBatch-372 loss:0.0159\n",
      "epoch[3/5], MiniBatch-373 loss:0.0158\n",
      "epoch[3/5], MiniBatch-374 loss:0.0163\n",
      "epoch[3/5], MiniBatch-375 loss:0.0157\n",
      "epoch[3/5], MiniBatch-376 loss:0.0155\n",
      "epoch[3/5], MiniBatch-377 loss:0.0159\n",
      "epoch[3/5], MiniBatch-378 loss:0.0159\n",
      "epoch[3/5], MiniBatch-379 loss:0.0156\n",
      "epoch[3/5], MiniBatch-380 loss:0.0155\n",
      "epoch[3/5], MiniBatch-381 loss:0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:42<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3/5], [-----TEST------] loss:0.0330  Accur:0.9289\n",
      "epoch[4/5], MiniBatch-1 loss:0.0152\n",
      "epoch[4/5], MiniBatch-2 loss:0.0149\n",
      "epoch[4/5], MiniBatch-3 loss:0.0152\n",
      "epoch[4/5], MiniBatch-4 loss:0.0155\n",
      "epoch[4/5], MiniBatch-5 loss:0.0152\n",
      "epoch[4/5], MiniBatch-6 loss:0.0154\n",
      "epoch[4/5], MiniBatch-7 loss:0.0151\n",
      "epoch[4/5], MiniBatch-8 loss:0.0152\n",
      "epoch[4/5], MiniBatch-9 loss:0.0152\n",
      "epoch[4/5], MiniBatch-10 loss:0.0152\n",
      "epoch[4/5], MiniBatch-11 loss:0.0153\n",
      "epoch[4/5], MiniBatch-12 loss:0.0152\n",
      "epoch[4/5], MiniBatch-13 loss:0.0155\n",
      "epoch[4/5], MiniBatch-14 loss:0.0152\n",
      "epoch[4/5], MiniBatch-15 loss:0.0157\n",
      "epoch[4/5], MiniBatch-16 loss:0.0156\n",
      "epoch[4/5], MiniBatch-17 loss:0.0157\n",
      "epoch[4/5], MiniBatch-18 loss:0.0151\n",
      "epoch[4/5], MiniBatch-19 loss:0.0153\n",
      "epoch[4/5], MiniBatch-20 loss:0.0149\n",
      "epoch[4/5], MiniBatch-21 loss:0.0153\n",
      "epoch[4/5], MiniBatch-22 loss:0.0156\n",
      "epoch[4/5], MiniBatch-23 loss:0.0156\n",
      "epoch[4/5], MiniBatch-24 loss:0.0150\n",
      "epoch[4/5], MiniBatch-25 loss:0.0151\n",
      "epoch[4/5], MiniBatch-26 loss:0.0151\n",
      "epoch[4/5], MiniBatch-27 loss:0.0158\n",
      "epoch[4/5], MiniBatch-28 loss:0.0153\n",
      "epoch[4/5], MiniBatch-29 loss:0.0156\n",
      "epoch[4/5], MiniBatch-30 loss:0.0152\n",
      "epoch[4/5], MiniBatch-31 loss:0.0152\n",
      "epoch[4/5], MiniBatch-32 loss:0.0150\n",
      "epoch[4/5], MiniBatch-33 loss:0.0153\n",
      "epoch[4/5], MiniBatch-34 loss:0.0154\n",
      "epoch[4/5], MiniBatch-35 loss:0.0152\n",
      "epoch[4/5], MiniBatch-36 loss:0.0153\n",
      "epoch[4/5], MiniBatch-37 loss:0.0156\n",
      "epoch[4/5], MiniBatch-38 loss:0.0153\n",
      "epoch[4/5], MiniBatch-39 loss:0.0156\n",
      "epoch[4/5], MiniBatch-40 loss:0.0156\n",
      "epoch[4/5], MiniBatch-41 loss:0.0152\n",
      "epoch[4/5], MiniBatch-42 loss:0.0155\n",
      "epoch[4/5], MiniBatch-43 loss:0.0151\n",
      "epoch[4/5], MiniBatch-44 loss:0.0161\n",
      "epoch[4/5], MiniBatch-45 loss:0.0155\n",
      "epoch[4/5], MiniBatch-46 loss:0.0151\n",
      "epoch[4/5], MiniBatch-47 loss:0.0155\n",
      "epoch[4/5], MiniBatch-48 loss:0.0152\n",
      "epoch[4/5], MiniBatch-49 loss:0.0154\n",
      "epoch[4/5], MiniBatch-50 loss:0.0150\n",
      "epoch[4/5], MiniBatch-51 loss:0.0152\n",
      "epoch[4/5], MiniBatch-52 loss:0.0153\n",
      "epoch[4/5], MiniBatch-53 loss:0.0153\n",
      "epoch[4/5], MiniBatch-54 loss:0.0151\n",
      "epoch[4/5], MiniBatch-55 loss:0.0152\n",
      "epoch[4/5], MiniBatch-56 loss:0.0154\n",
      "epoch[4/5], MiniBatch-57 loss:0.0153\n",
      "epoch[4/5], MiniBatch-58 loss:0.0158\n",
      "epoch[4/5], MiniBatch-59 loss:0.0153\n",
      "epoch[4/5], MiniBatch-60 loss:0.0154\n",
      "epoch[4/5], MiniBatch-61 loss:0.0157\n",
      "epoch[4/5], MiniBatch-62 loss:0.0152\n",
      "epoch[4/5], MiniBatch-63 loss:0.0153\n",
      "epoch[4/5], MiniBatch-64 loss:0.0154\n",
      "epoch[4/5], MiniBatch-65 loss:0.0155\n",
      "epoch[4/5], MiniBatch-66 loss:0.0155\n",
      "epoch[4/5], MiniBatch-67 loss:0.0150\n",
      "epoch[4/5], MiniBatch-68 loss:0.0155\n",
      "epoch[4/5], MiniBatch-69 loss:0.0153\n",
      "epoch[4/5], MiniBatch-70 loss:0.0150\n",
      "epoch[4/5], MiniBatch-71 loss:0.0156\n",
      "epoch[4/5], MiniBatch-72 loss:0.0156\n",
      "epoch[4/5], MiniBatch-73 loss:0.0153\n",
      "epoch[4/5], MiniBatch-74 loss:0.0153\n",
      "epoch[4/5], MiniBatch-75 loss:0.0154\n",
      "epoch[4/5], MiniBatch-76 loss:0.0153\n",
      "epoch[4/5], MiniBatch-77 loss:0.0155\n",
      "epoch[4/5], MiniBatch-78 loss:0.0155\n",
      "epoch[4/5], MiniBatch-79 loss:0.0155\n",
      "epoch[4/5], MiniBatch-80 loss:0.0158\n",
      "epoch[4/5], MiniBatch-81 loss:0.0153\n",
      "epoch[4/5], MiniBatch-82 loss:0.0155\n",
      "epoch[4/5], MiniBatch-83 loss:0.0163\n",
      "epoch[4/5], MiniBatch-84 loss:0.0156\n",
      "epoch[4/5], MiniBatch-85 loss:0.0153\n",
      "epoch[4/5], MiniBatch-86 loss:0.0154\n",
      "epoch[4/5], MiniBatch-87 loss:0.0156\n",
      "epoch[4/5], MiniBatch-88 loss:0.0153\n",
      "epoch[4/5], MiniBatch-89 loss:0.0153\n",
      "epoch[4/5], MiniBatch-90 loss:0.0153\n",
      "epoch[4/5], MiniBatch-91 loss:0.0159\n",
      "epoch[4/5], MiniBatch-92 loss:0.0157\n",
      "epoch[4/5], MiniBatch-93 loss:0.0154\n",
      "epoch[4/5], MiniBatch-94 loss:0.0154\n",
      "epoch[4/5], MiniBatch-95 loss:0.0155\n",
      "epoch[4/5], MiniBatch-96 loss:0.0153\n",
      "epoch[4/5], MiniBatch-97 loss:0.0155\n",
      "epoch[4/5], MiniBatch-98 loss:0.0155\n",
      "epoch[4/5], MiniBatch-99 loss:0.0150\n",
      "epoch[4/5], MiniBatch-100 loss:0.0156\n",
      "epoch[4/5], MiniBatch-101 loss:0.0151\n",
      "epoch[4/5], MiniBatch-102 loss:0.0152\n",
      "epoch[4/5], MiniBatch-103 loss:0.0154\n",
      "epoch[4/5], MiniBatch-104 loss:0.0153\n",
      "epoch[4/5], MiniBatch-105 loss:0.0152\n",
      "epoch[4/5], MiniBatch-106 loss:0.0156\n",
      "epoch[4/5], MiniBatch-107 loss:0.0157\n",
      "epoch[4/5], MiniBatch-108 loss:0.0153\n",
      "epoch[4/5], MiniBatch-109 loss:0.0155\n",
      "epoch[4/5], MiniBatch-110 loss:0.0155\n",
      "epoch[4/5], MiniBatch-111 loss:0.0154\n",
      "epoch[4/5], MiniBatch-112 loss:0.0154\n",
      "epoch[4/5], MiniBatch-113 loss:0.0155\n",
      "epoch[4/5], MiniBatch-114 loss:0.0157\n",
      "epoch[4/5], MiniBatch-115 loss:0.0152\n",
      "epoch[4/5], MiniBatch-116 loss:0.0157\n",
      "epoch[4/5], MiniBatch-117 loss:0.0160\n",
      "epoch[4/5], MiniBatch-118 loss:0.0156\n",
      "epoch[4/5], MiniBatch-119 loss:0.0154\n",
      "epoch[4/5], MiniBatch-120 loss:0.0155\n",
      "epoch[4/5], MiniBatch-121 loss:0.0154\n",
      "epoch[4/5], MiniBatch-122 loss:0.0157\n",
      "epoch[4/5], MiniBatch-123 loss:0.0150\n",
      "epoch[4/5], MiniBatch-124 loss:0.0154\n",
      "epoch[4/5], MiniBatch-125 loss:0.0152\n",
      "epoch[4/5], MiniBatch-126 loss:0.0154\n",
      "epoch[4/5], MiniBatch-127 loss:0.0150\n",
      "epoch[4/5], MiniBatch-128 loss:0.0153\n",
      "epoch[4/5], MiniBatch-129 loss:0.0153\n",
      "epoch[4/5], MiniBatch-130 loss:0.0158\n",
      "epoch[4/5], MiniBatch-131 loss:0.0156\n",
      "epoch[4/5], MiniBatch-132 loss:0.0159\n",
      "epoch[4/5], MiniBatch-133 loss:0.0155\n",
      "epoch[4/5], MiniBatch-134 loss:0.0151\n",
      "epoch[4/5], MiniBatch-135 loss:0.0154\n",
      "epoch[4/5], MiniBatch-136 loss:0.0154\n",
      "epoch[4/5], MiniBatch-137 loss:0.0154\n",
      "epoch[4/5], MiniBatch-138 loss:0.0152\n",
      "epoch[4/5], MiniBatch-139 loss:0.0153\n",
      "epoch[4/5], MiniBatch-140 loss:0.0151\n",
      "epoch[4/5], MiniBatch-141 loss:0.0155\n",
      "epoch[4/5], MiniBatch-142 loss:0.0157\n",
      "epoch[4/5], MiniBatch-143 loss:0.0155\n",
      "epoch[4/5], MiniBatch-144 loss:0.0152\n",
      "epoch[4/5], MiniBatch-145 loss:0.0155\n",
      "epoch[4/5], MiniBatch-146 loss:0.0158\n",
      "epoch[4/5], MiniBatch-147 loss:0.0156\n",
      "epoch[4/5], MiniBatch-148 loss:0.0158\n",
      "epoch[4/5], MiniBatch-149 loss:0.0154\n",
      "epoch[4/5], MiniBatch-150 loss:0.0152\n",
      "epoch[4/5], MiniBatch-151 loss:0.0157\n",
      "epoch[4/5], MiniBatch-152 loss:0.0156\n",
      "epoch[4/5], MiniBatch-153 loss:0.0154\n",
      "epoch[4/5], MiniBatch-154 loss:0.0157\n",
      "epoch[4/5], MiniBatch-155 loss:0.0159\n",
      "epoch[4/5], MiniBatch-156 loss:0.0155\n",
      "epoch[4/5], MiniBatch-157 loss:0.0156\n",
      "epoch[4/5], MiniBatch-158 loss:0.0155\n",
      "epoch[4/5], MiniBatch-159 loss:0.0154\n",
      "epoch[4/5], MiniBatch-160 loss:0.0154\n",
      "epoch[4/5], MiniBatch-161 loss:0.0157\n",
      "epoch[4/5], MiniBatch-162 loss:0.0162\n",
      "epoch[4/5], MiniBatch-163 loss:0.0153\n",
      "epoch[4/5], MiniBatch-164 loss:0.0155\n",
      "epoch[4/5], MiniBatch-165 loss:0.0152\n",
      "epoch[4/5], MiniBatch-166 loss:0.0152\n",
      "epoch[4/5], MiniBatch-167 loss:0.0160\n",
      "epoch[4/5], MiniBatch-168 loss:0.0159\n",
      "epoch[4/5], MiniBatch-169 loss:0.0154\n",
      "epoch[4/5], MiniBatch-170 loss:0.0154\n",
      "epoch[4/5], MiniBatch-171 loss:0.0152\n",
      "epoch[4/5], MiniBatch-172 loss:0.0157\n",
      "epoch[4/5], MiniBatch-173 loss:0.0156\n",
      "epoch[4/5], MiniBatch-174 loss:0.0153\n",
      "epoch[4/5], MiniBatch-175 loss:0.0156\n",
      "epoch[4/5], MiniBatch-176 loss:0.0159\n",
      "epoch[4/5], MiniBatch-177 loss:0.0161\n",
      "epoch[4/5], MiniBatch-178 loss:0.0153\n",
      "epoch[4/5], MiniBatch-179 loss:0.0151\n",
      "epoch[4/5], MiniBatch-180 loss:0.0156\n",
      "epoch[4/5], MiniBatch-181 loss:0.0156\n",
      "epoch[4/5], MiniBatch-182 loss:0.0152\n",
      "epoch[4/5], MiniBatch-183 loss:0.0157\n",
      "epoch[4/5], MiniBatch-184 loss:0.0158\n",
      "epoch[4/5], MiniBatch-185 loss:0.0160\n",
      "epoch[4/5], MiniBatch-186 loss:0.0157\n",
      "epoch[4/5], MiniBatch-187 loss:0.0155\n",
      "epoch[4/5], MiniBatch-188 loss:0.0158\n",
      "epoch[4/5], MiniBatch-189 loss:0.0154\n",
      "epoch[4/5], MiniBatch-190 loss:0.0153\n",
      "epoch[4/5], MiniBatch-191 loss:0.0156\n",
      "epoch[4/5], MiniBatch-192 loss:0.0156\n",
      "epoch[4/5], MiniBatch-193 loss:0.0156\n",
      "epoch[4/5], MiniBatch-194 loss:0.0160\n",
      "epoch[4/5], MiniBatch-195 loss:0.0158\n",
      "epoch[4/5], MiniBatch-196 loss:0.0157\n",
      "epoch[4/5], MiniBatch-197 loss:0.0155\n",
      "epoch[4/5], MiniBatch-198 loss:0.0155\n",
      "epoch[4/5], MiniBatch-199 loss:0.0153\n",
      "epoch[4/5], MiniBatch-200 loss:0.0158\n",
      "epoch[4/5], MiniBatch-201 loss:0.0159\n",
      "epoch[4/5], MiniBatch-202 loss:0.0156\n",
      "epoch[4/5], MiniBatch-203 loss:0.0157\n",
      "epoch[4/5], MiniBatch-204 loss:0.0153\n",
      "epoch[4/5], MiniBatch-205 loss:0.0154\n",
      "epoch[4/5], MiniBatch-206 loss:0.0152\n",
      "epoch[4/5], MiniBatch-207 loss:0.0153\n",
      "epoch[4/5], MiniBatch-208 loss:0.0161\n",
      "epoch[4/5], MiniBatch-209 loss:0.0160\n",
      "epoch[4/5], MiniBatch-210 loss:0.0155\n",
      "epoch[4/5], MiniBatch-211 loss:0.0155\n",
      "epoch[4/5], MiniBatch-212 loss:0.0158\n",
      "epoch[4/5], MiniBatch-213 loss:0.0154\n",
      "epoch[4/5], MiniBatch-214 loss:0.0155\n",
      "epoch[4/5], MiniBatch-215 loss:0.0154\n",
      "epoch[4/5], MiniBatch-216 loss:0.0150\n",
      "epoch[4/5], MiniBatch-217 loss:0.0153\n",
      "epoch[4/5], MiniBatch-218 loss:0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[4/5], MiniBatch-219 loss:0.0158\n",
      "epoch[4/5], MiniBatch-220 loss:0.0154\n",
      "epoch[4/5], MiniBatch-221 loss:0.0161\n",
      "epoch[4/5], MiniBatch-222 loss:0.0155\n",
      "epoch[4/5], MiniBatch-223 loss:0.0158\n",
      "epoch[4/5], MiniBatch-224 loss:0.0156\n",
      "epoch[4/5], MiniBatch-225 loss:0.0157\n",
      "epoch[4/5], MiniBatch-226 loss:0.0155\n",
      "epoch[4/5], MiniBatch-227 loss:0.0153\n",
      "epoch[4/5], MiniBatch-228 loss:0.0157\n",
      "epoch[4/5], MiniBatch-229 loss:0.0156\n",
      "epoch[4/5], MiniBatch-230 loss:0.0157\n",
      "epoch[4/5], MiniBatch-231 loss:0.0157\n",
      "epoch[4/5], MiniBatch-232 loss:0.0157\n",
      "epoch[4/5], MiniBatch-233 loss:0.0160\n",
      "epoch[4/5], MiniBatch-234 loss:0.0156\n",
      "epoch[4/5], MiniBatch-235 loss:0.0156\n",
      "epoch[4/5], MiniBatch-236 loss:0.0160\n",
      "epoch[4/5], MiniBatch-237 loss:0.0156\n",
      "epoch[4/5], MiniBatch-238 loss:0.0159\n",
      "epoch[4/5], MiniBatch-239 loss:0.0160\n",
      "epoch[4/5], MiniBatch-240 loss:0.0153\n",
      "epoch[4/5], MiniBatch-241 loss:0.0157\n",
      "epoch[4/5], MiniBatch-242 loss:0.0154\n",
      "epoch[4/5], MiniBatch-243 loss:0.0160\n",
      "epoch[4/5], MiniBatch-244 loss:0.0158\n",
      "epoch[4/5], MiniBatch-245 loss:0.0153\n",
      "epoch[4/5], MiniBatch-246 loss:0.0165\n",
      "epoch[4/5], MiniBatch-247 loss:0.0160\n",
      "epoch[4/5], MiniBatch-248 loss:0.0154\n",
      "epoch[4/5], MiniBatch-249 loss:0.0153\n",
      "epoch[4/5], MiniBatch-250 loss:0.0154\n",
      "epoch[4/5], MiniBatch-251 loss:0.0164\n",
      "epoch[4/5], MiniBatch-252 loss:0.0157\n",
      "epoch[4/5], MiniBatch-253 loss:0.0157\n",
      "epoch[4/5], MiniBatch-254 loss:0.0153\n",
      "epoch[4/5], MiniBatch-255 loss:0.0155\n",
      "epoch[4/5], MiniBatch-256 loss:0.0156\n",
      "epoch[4/5], MiniBatch-257 loss:0.0156\n",
      "epoch[4/5], MiniBatch-258 loss:0.0157\n",
      "epoch[4/5], MiniBatch-259 loss:0.0158\n",
      "epoch[4/5], MiniBatch-260 loss:0.0157\n",
      "epoch[4/5], MiniBatch-261 loss:0.0154\n",
      "epoch[4/5], MiniBatch-262 loss:0.0157\n",
      "epoch[4/5], MiniBatch-263 loss:0.0157\n",
      "epoch[4/5], MiniBatch-264 loss:0.0159\n",
      "epoch[4/5], MiniBatch-265 loss:0.0160\n",
      "epoch[4/5], MiniBatch-266 loss:0.0156\n",
      "epoch[4/5], MiniBatch-267 loss:0.0162\n",
      "epoch[4/5], MiniBatch-268 loss:0.0154\n",
      "epoch[4/5], MiniBatch-269 loss:0.0160\n",
      "epoch[4/5], MiniBatch-270 loss:0.0154\n",
      "epoch[4/5], MiniBatch-271 loss:0.0157\n",
      "epoch[4/5], MiniBatch-272 loss:0.0156\n",
      "epoch[4/5], MiniBatch-273 loss:0.0160\n",
      "epoch[4/5], MiniBatch-274 loss:0.0159\n",
      "epoch[4/5], MiniBatch-275 loss:0.0156\n",
      "epoch[4/5], MiniBatch-276 loss:0.0158\n",
      "epoch[4/5], MiniBatch-277 loss:0.0160\n",
      "epoch[4/5], MiniBatch-278 loss:0.0156\n",
      "epoch[4/5], MiniBatch-279 loss:0.0158\n",
      "epoch[4/5], MiniBatch-280 loss:0.0156\n",
      "epoch[4/5], MiniBatch-281 loss:0.0159\n",
      "epoch[4/5], MiniBatch-282 loss:0.0158\n",
      "epoch[4/5], MiniBatch-283 loss:0.0158\n",
      "epoch[4/5], MiniBatch-284 loss:0.0155\n",
      "epoch[4/5], MiniBatch-285 loss:0.0160\n",
      "epoch[4/5], MiniBatch-286 loss:0.0154\n",
      "epoch[4/5], MiniBatch-287 loss:0.0162\n",
      "epoch[4/5], MiniBatch-288 loss:0.0155\n",
      "epoch[4/5], MiniBatch-289 loss:0.0159\n",
      "epoch[4/5], MiniBatch-290 loss:0.0162\n",
      "epoch[4/5], MiniBatch-291 loss:0.0163\n",
      "epoch[4/5], MiniBatch-292 loss:0.0153\n",
      "epoch[4/5], MiniBatch-293 loss:0.0155\n",
      "epoch[4/5], MiniBatch-294 loss:0.0158\n",
      "epoch[4/5], MiniBatch-295 loss:0.0161\n",
      "epoch[4/5], MiniBatch-296 loss:0.0159\n",
      "epoch[4/5], MiniBatch-297 loss:0.0161\n",
      "epoch[4/5], MiniBatch-298 loss:0.0158\n",
      "epoch[4/5], MiniBatch-299 loss:0.0157\n",
      "epoch[4/5], MiniBatch-300 loss:0.0156\n",
      "epoch[4/5], MiniBatch-301 loss:0.0156\n",
      "epoch[4/5], MiniBatch-302 loss:0.0155\n",
      "epoch[4/5], MiniBatch-303 loss:0.0157\n",
      "epoch[4/5], MiniBatch-304 loss:0.0159\n",
      "epoch[4/5], MiniBatch-305 loss:0.0157\n",
      "epoch[4/5], MiniBatch-306 loss:0.0157\n",
      "epoch[4/5], MiniBatch-307 loss:0.0159\n",
      "epoch[4/5], MiniBatch-308 loss:0.0162\n",
      "epoch[4/5], MiniBatch-309 loss:0.0153\n",
      "epoch[4/5], MiniBatch-310 loss:0.0157\n",
      "epoch[4/5], MiniBatch-311 loss:0.0162\n",
      "epoch[4/5], MiniBatch-312 loss:0.0159\n",
      "epoch[4/5], MiniBatch-313 loss:0.0164\n",
      "epoch[4/5], MiniBatch-314 loss:0.0155\n",
      "epoch[4/5], MiniBatch-315 loss:0.0157\n",
      "epoch[4/5], MiniBatch-316 loss:0.0159\n",
      "epoch[4/5], MiniBatch-317 loss:0.0154\n",
      "epoch[4/5], MiniBatch-318 loss:0.0155\n",
      "epoch[4/5], MiniBatch-319 loss:0.0160\n",
      "epoch[4/5], MiniBatch-320 loss:0.0161\n",
      "epoch[4/5], MiniBatch-321 loss:0.0157\n",
      "epoch[4/5], MiniBatch-322 loss:0.0157\n",
      "epoch[4/5], MiniBatch-323 loss:0.0159\n",
      "epoch[4/5], MiniBatch-324 loss:0.0157\n",
      "epoch[4/5], MiniBatch-325 loss:0.0156\n",
      "epoch[4/5], MiniBatch-326 loss:0.0155\n",
      "epoch[4/5], MiniBatch-327 loss:0.0159\n",
      "epoch[4/5], MiniBatch-328 loss:0.0155\n",
      "epoch[4/5], MiniBatch-329 loss:0.0162\n",
      "epoch[4/5], MiniBatch-330 loss:0.0163\n",
      "epoch[4/5], MiniBatch-331 loss:0.0153\n",
      "epoch[4/5], MiniBatch-332 loss:0.0154\n",
      "epoch[4/5], MiniBatch-333 loss:0.0158\n",
      "epoch[4/5], MiniBatch-334 loss:0.0155\n",
      "epoch[4/5], MiniBatch-335 loss:0.0151\n",
      "epoch[4/5], MiniBatch-336 loss:0.0157\n",
      "epoch[4/5], MiniBatch-337 loss:0.0157\n",
      "epoch[4/5], MiniBatch-338 loss:0.0157\n",
      "epoch[4/5], MiniBatch-339 loss:0.0151\n",
      "epoch[4/5], MiniBatch-340 loss:0.0160\n",
      "epoch[4/5], MiniBatch-341 loss:0.0156\n",
      "epoch[4/5], MiniBatch-342 loss:0.0155\n",
      "epoch[4/5], MiniBatch-343 loss:0.0156\n",
      "epoch[4/5], MiniBatch-344 loss:0.0156\n",
      "epoch[4/5], MiniBatch-345 loss:0.0157\n",
      "epoch[4/5], MiniBatch-346 loss:0.0158\n",
      "epoch[4/5], MiniBatch-347 loss:0.0158\n",
      "epoch[4/5], MiniBatch-348 loss:0.0161\n",
      "epoch[4/5], MiniBatch-349 loss:0.0155\n",
      "epoch[4/5], MiniBatch-350 loss:0.0156\n",
      "epoch[4/5], MiniBatch-351 loss:0.0159\n",
      "epoch[4/5], MiniBatch-352 loss:0.0160\n",
      "epoch[4/5], MiniBatch-353 loss:0.0159\n",
      "epoch[4/5], MiniBatch-354 loss:0.0156\n",
      "epoch[4/5], MiniBatch-355 loss:0.0159\n",
      "epoch[4/5], MiniBatch-356 loss:0.0159\n",
      "epoch[4/5], MiniBatch-357 loss:0.0155\n",
      "epoch[4/5], MiniBatch-358 loss:0.0157\n",
      "epoch[4/5], MiniBatch-359 loss:0.0159\n",
      "epoch[4/5], MiniBatch-360 loss:0.0153\n",
      "epoch[4/5], MiniBatch-361 loss:0.0155\n",
      "epoch[4/5], MiniBatch-362 loss:0.0158\n",
      "epoch[4/5], MiniBatch-363 loss:0.0156\n",
      "epoch[4/5], MiniBatch-364 loss:0.0160\n",
      "epoch[4/5], MiniBatch-365 loss:0.0159\n",
      "epoch[4/5], MiniBatch-366 loss:0.0161\n",
      "epoch[4/5], MiniBatch-367 loss:0.0157\n",
      "epoch[4/5], MiniBatch-368 loss:0.0153\n",
      "epoch[4/5], MiniBatch-369 loss:0.0158\n",
      "epoch[4/5], MiniBatch-370 loss:0.0153\n",
      "epoch[4/5], MiniBatch-371 loss:0.0155\n",
      "epoch[4/5], MiniBatch-372 loss:0.0159\n",
      "epoch[4/5], MiniBatch-373 loss:0.0155\n",
      "epoch[4/5], MiniBatch-374 loss:0.0153\n",
      "epoch[4/5], MiniBatch-375 loss:0.0157\n",
      "epoch[4/5], MiniBatch-376 loss:0.0159\n",
      "epoch[4/5], MiniBatch-377 loss:0.0158\n",
      "epoch[4/5], MiniBatch-378 loss:0.0159\n",
      "epoch[4/5], MiniBatch-379 loss:0.0159\n",
      "epoch[4/5], MiniBatch-380 loss:0.0156\n",
      "epoch[4/5], MiniBatch-381 loss:0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:41<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[4/5], [-----TEST------] loss:0.0331  Accur:0.9327\n",
      "epoch[5/5], MiniBatch-1 loss:0.0156\n",
      "epoch[5/5], MiniBatch-2 loss:0.0151\n",
      "epoch[5/5], MiniBatch-3 loss:0.0155\n",
      "epoch[5/5], MiniBatch-4 loss:0.0158\n",
      "epoch[5/5], MiniBatch-5 loss:0.0154\n",
      "epoch[5/5], MiniBatch-6 loss:0.0156\n",
      "epoch[5/5], MiniBatch-7 loss:0.0155\n",
      "epoch[5/5], MiniBatch-8 loss:0.0154\n",
      "epoch[5/5], MiniBatch-9 loss:0.0156\n",
      "epoch[5/5], MiniBatch-10 loss:0.0152\n",
      "epoch[5/5], MiniBatch-11 loss:0.0157\n",
      "epoch[5/5], MiniBatch-12 loss:0.0152\n",
      "epoch[5/5], MiniBatch-13 loss:0.0155\n",
      "epoch[5/5], MiniBatch-14 loss:0.0152\n",
      "epoch[5/5], MiniBatch-15 loss:0.0151\n",
      "epoch[5/5], MiniBatch-16 loss:0.0149\n",
      "epoch[5/5], MiniBatch-17 loss:0.0155\n",
      "epoch[5/5], MiniBatch-18 loss:0.0155\n",
      "epoch[5/5], MiniBatch-19 loss:0.0152\n",
      "epoch[5/5], MiniBatch-20 loss:0.0152\n",
      "epoch[5/5], MiniBatch-21 loss:0.0154\n",
      "epoch[5/5], MiniBatch-22 loss:0.0159\n",
      "epoch[5/5], MiniBatch-23 loss:0.0155\n",
      "epoch[5/5], MiniBatch-24 loss:0.0155\n",
      "epoch[5/5], MiniBatch-25 loss:0.0157\n",
      "epoch[5/5], MiniBatch-26 loss:0.0152\n",
      "epoch[5/5], MiniBatch-27 loss:0.0154\n",
      "epoch[5/5], MiniBatch-28 loss:0.0148\n",
      "epoch[5/5], MiniBatch-29 loss:0.0156\n",
      "epoch[5/5], MiniBatch-30 loss:0.0156\n",
      "epoch[5/5], MiniBatch-31 loss:0.0148\n",
      "epoch[5/5], MiniBatch-32 loss:0.0157\n",
      "epoch[5/5], MiniBatch-33 loss:0.0151\n",
      "epoch[5/5], MiniBatch-34 loss:0.0152\n",
      "epoch[5/5], MiniBatch-35 loss:0.0153\n",
      "epoch[5/5], MiniBatch-36 loss:0.0150\n",
      "epoch[5/5], MiniBatch-37 loss:0.0156\n",
      "epoch[5/5], MiniBatch-38 loss:0.0153\n",
      "epoch[5/5], MiniBatch-39 loss:0.0151\n",
      "epoch[5/5], MiniBatch-40 loss:0.0157\n",
      "epoch[5/5], MiniBatch-41 loss:0.0156\n",
      "epoch[5/5], MiniBatch-42 loss:0.0150\n",
      "epoch[5/5], MiniBatch-43 loss:0.0152\n",
      "epoch[5/5], MiniBatch-44 loss:0.0151\n",
      "epoch[5/5], MiniBatch-45 loss:0.0158\n",
      "epoch[5/5], MiniBatch-46 loss:0.0154\n",
      "epoch[5/5], MiniBatch-47 loss:0.0155\n",
      "epoch[5/5], MiniBatch-48 loss:0.0151\n",
      "epoch[5/5], MiniBatch-49 loss:0.0159\n",
      "epoch[5/5], MiniBatch-50 loss:0.0154\n",
      "epoch[5/5], MiniBatch-51 loss:0.0150\n",
      "epoch[5/5], MiniBatch-52 loss:0.0158\n",
      "epoch[5/5], MiniBatch-53 loss:0.0153\n",
      "epoch[5/5], MiniBatch-54 loss:0.0154\n",
      "epoch[5/5], MiniBatch-55 loss:0.0156\n",
      "epoch[5/5], MiniBatch-56 loss:0.0156\n",
      "epoch[5/5], MiniBatch-57 loss:0.0151\n",
      "epoch[5/5], MiniBatch-58 loss:0.0156\n",
      "epoch[5/5], MiniBatch-59 loss:0.0148\n",
      "epoch[5/5], MiniBatch-60 loss:0.0152\n",
      "epoch[5/5], MiniBatch-61 loss:0.0153\n",
      "epoch[5/5], MiniBatch-62 loss:0.0148\n",
      "epoch[5/5], MiniBatch-63 loss:0.0154\n",
      "epoch[5/5], MiniBatch-64 loss:0.0150\n",
      "epoch[5/5], MiniBatch-65 loss:0.0156\n",
      "epoch[5/5], MiniBatch-66 loss:0.0154\n",
      "epoch[5/5], MiniBatch-67 loss:0.0156\n",
      "epoch[5/5], MiniBatch-68 loss:0.0155\n",
      "epoch[5/5], MiniBatch-69 loss:0.0153\n",
      "epoch[5/5], MiniBatch-70 loss:0.0153\n",
      "epoch[5/5], MiniBatch-71 loss:0.0151\n",
      "epoch[5/5], MiniBatch-72 loss:0.0155\n",
      "epoch[5/5], MiniBatch-73 loss:0.0152\n",
      "epoch[5/5], MiniBatch-74 loss:0.0154\n",
      "epoch[5/5], MiniBatch-75 loss:0.0154\n",
      "epoch[5/5], MiniBatch-76 loss:0.0151\n",
      "epoch[5/5], MiniBatch-77 loss:0.0149\n",
      "epoch[5/5], MiniBatch-78 loss:0.0148\n",
      "epoch[5/5], MiniBatch-79 loss:0.0150\n",
      "epoch[5/5], MiniBatch-80 loss:0.0150\n",
      "epoch[5/5], MiniBatch-81 loss:0.0155\n",
      "epoch[5/5], MiniBatch-82 loss:0.0154\n",
      "epoch[5/5], MiniBatch-83 loss:0.0153\n",
      "epoch[5/5], MiniBatch-84 loss:0.0157\n",
      "epoch[5/5], MiniBatch-85 loss:0.0153\n",
      "epoch[5/5], MiniBatch-86 loss:0.0154\n",
      "epoch[5/5], MiniBatch-87 loss:0.0154\n",
      "epoch[5/5], MiniBatch-88 loss:0.0154\n",
      "epoch[5/5], MiniBatch-89 loss:0.0152\n",
      "epoch[5/5], MiniBatch-90 loss:0.0154\n",
      "epoch[5/5], MiniBatch-91 loss:0.0148\n",
      "epoch[5/5], MiniBatch-92 loss:0.0156\n",
      "epoch[5/5], MiniBatch-93 loss:0.0154\n",
      "epoch[5/5], MiniBatch-94 loss:0.0156\n",
      "epoch[5/5], MiniBatch-95 loss:0.0153\n",
      "epoch[5/5], MiniBatch-96 loss:0.0149\n",
      "epoch[5/5], MiniBatch-97 loss:0.0154\n",
      "epoch[5/5], MiniBatch-98 loss:0.0159\n",
      "epoch[5/5], MiniBatch-99 loss:0.0152\n",
      "epoch[5/5], MiniBatch-100 loss:0.0152\n",
      "epoch[5/5], MiniBatch-101 loss:0.0156\n",
      "epoch[5/5], MiniBatch-102 loss:0.0157\n",
      "epoch[5/5], MiniBatch-103 loss:0.0156\n",
      "epoch[5/5], MiniBatch-104 loss:0.0150\n",
      "epoch[5/5], MiniBatch-105 loss:0.0153\n",
      "epoch[5/5], MiniBatch-106 loss:0.0149\n",
      "epoch[5/5], MiniBatch-107 loss:0.0156\n",
      "epoch[5/5], MiniBatch-108 loss:0.0152\n",
      "epoch[5/5], MiniBatch-109 loss:0.0153\n",
      "epoch[5/5], MiniBatch-110 loss:0.0155\n",
      "epoch[5/5], MiniBatch-111 loss:0.0149\n",
      "epoch[5/5], MiniBatch-112 loss:0.0156\n",
      "epoch[5/5], MiniBatch-113 loss:0.0154\n",
      "epoch[5/5], MiniBatch-114 loss:0.0153\n",
      "epoch[5/5], MiniBatch-115 loss:0.0156\n",
      "epoch[5/5], MiniBatch-116 loss:0.0156\n",
      "epoch[5/5], MiniBatch-117 loss:0.0155\n",
      "epoch[5/5], MiniBatch-118 loss:0.0156\n",
      "epoch[5/5], MiniBatch-119 loss:0.0154\n",
      "epoch[5/5], MiniBatch-120 loss:0.0151\n",
      "epoch[5/5], MiniBatch-121 loss:0.0154\n",
      "epoch[5/5], MiniBatch-122 loss:0.0155\n",
      "epoch[5/5], MiniBatch-123 loss:0.0151\n",
      "epoch[5/5], MiniBatch-124 loss:0.0155\n",
      "epoch[5/5], MiniBatch-125 loss:0.0158\n",
      "epoch[5/5], MiniBatch-126 loss:0.0155\n",
      "epoch[5/5], MiniBatch-127 loss:0.0158\n",
      "epoch[5/5], MiniBatch-128 loss:0.0155\n",
      "epoch[5/5], MiniBatch-129 loss:0.0157\n",
      "epoch[5/5], MiniBatch-130 loss:0.0155\n",
      "epoch[5/5], MiniBatch-131 loss:0.0157\n",
      "epoch[5/5], MiniBatch-132 loss:0.0150\n",
      "epoch[5/5], MiniBatch-133 loss:0.0152\n",
      "epoch[5/5], MiniBatch-134 loss:0.0154\n",
      "epoch[5/5], MiniBatch-135 loss:0.0155\n",
      "epoch[5/5], MiniBatch-136 loss:0.0153\n",
      "epoch[5/5], MiniBatch-137 loss:0.0161\n",
      "epoch[5/5], MiniBatch-138 loss:0.0156\n",
      "epoch[5/5], MiniBatch-139 loss:0.0153\n",
      "epoch[5/5], MiniBatch-140 loss:0.0157\n",
      "epoch[5/5], MiniBatch-141 loss:0.0156\n",
      "epoch[5/5], MiniBatch-142 loss:0.0154\n",
      "epoch[5/5], MiniBatch-143 loss:0.0154\n",
      "epoch[5/5], MiniBatch-144 loss:0.0154\n",
      "epoch[5/5], MiniBatch-145 loss:0.0156\n",
      "epoch[5/5], MiniBatch-146 loss:0.0154\n",
      "epoch[5/5], MiniBatch-147 loss:0.0155\n",
      "epoch[5/5], MiniBatch-148 loss:0.0159\n",
      "epoch[5/5], MiniBatch-149 loss:0.0155\n",
      "epoch[5/5], MiniBatch-150 loss:0.0155\n",
      "epoch[5/5], MiniBatch-151 loss:0.0156\n",
      "epoch[5/5], MiniBatch-152 loss:0.0158\n",
      "epoch[5/5], MiniBatch-153 loss:0.0151\n",
      "epoch[5/5], MiniBatch-154 loss:0.0157\n",
      "epoch[5/5], MiniBatch-155 loss:0.0157\n",
      "epoch[5/5], MiniBatch-156 loss:0.0154\n",
      "epoch[5/5], MiniBatch-157 loss:0.0157\n",
      "epoch[5/5], MiniBatch-158 loss:0.0155\n",
      "epoch[5/5], MiniBatch-159 loss:0.0153\n",
      "epoch[5/5], MiniBatch-160 loss:0.0154\n",
      "epoch[5/5], MiniBatch-161 loss:0.0155\n",
      "epoch[5/5], MiniBatch-162 loss:0.0156\n",
      "epoch[5/5], MiniBatch-163 loss:0.0160\n",
      "epoch[5/5], MiniBatch-164 loss:0.0152\n",
      "epoch[5/5], MiniBatch-165 loss:0.0152\n",
      "epoch[5/5], MiniBatch-166 loss:0.0152\n",
      "epoch[5/5], MiniBatch-167 loss:0.0156\n",
      "epoch[5/5], MiniBatch-168 loss:0.0153\n",
      "epoch[5/5], MiniBatch-169 loss:0.0156\n",
      "epoch[5/5], MiniBatch-170 loss:0.0153\n",
      "epoch[5/5], MiniBatch-171 loss:0.0154\n",
      "epoch[5/5], MiniBatch-172 loss:0.0153\n",
      "epoch[5/5], MiniBatch-173 loss:0.0160\n",
      "epoch[5/5], MiniBatch-174 loss:0.0154\n",
      "epoch[5/5], MiniBatch-175 loss:0.0161\n",
      "epoch[5/5], MiniBatch-176 loss:0.0156\n",
      "epoch[5/5], MiniBatch-177 loss:0.0153\n",
      "epoch[5/5], MiniBatch-178 loss:0.0155\n",
      "epoch[5/5], MiniBatch-179 loss:0.0156\n",
      "epoch[5/5], MiniBatch-180 loss:0.0154\n",
      "epoch[5/5], MiniBatch-181 loss:0.0151\n",
      "epoch[5/5], MiniBatch-182 loss:0.0156\n",
      "epoch[5/5], MiniBatch-183 loss:0.0157\n",
      "epoch[5/5], MiniBatch-184 loss:0.0156\n",
      "epoch[5/5], MiniBatch-185 loss:0.0158\n",
      "epoch[5/5], MiniBatch-186 loss:0.0158\n",
      "epoch[5/5], MiniBatch-187 loss:0.0157\n",
      "epoch[5/5], MiniBatch-188 loss:0.0158\n",
      "epoch[5/5], MiniBatch-189 loss:0.0158\n",
      "epoch[5/5], MiniBatch-190 loss:0.0155\n",
      "epoch[5/5], MiniBatch-191 loss:0.0154\n",
      "epoch[5/5], MiniBatch-192 loss:0.0156\n",
      "epoch[5/5], MiniBatch-193 loss:0.0161\n",
      "epoch[5/5], MiniBatch-194 loss:0.0156\n",
      "epoch[5/5], MiniBatch-195 loss:0.0161\n",
      "epoch[5/5], MiniBatch-196 loss:0.0158\n",
      "epoch[5/5], MiniBatch-197 loss:0.0162\n",
      "epoch[5/5], MiniBatch-198 loss:0.0160\n",
      "epoch[5/5], MiniBatch-199 loss:0.0155\n",
      "epoch[5/5], MiniBatch-200 loss:0.0151\n",
      "epoch[5/5], MiniBatch-201 loss:0.0154\n",
      "epoch[5/5], MiniBatch-202 loss:0.0157\n",
      "epoch[5/5], MiniBatch-203 loss:0.0156\n",
      "epoch[5/5], MiniBatch-204 loss:0.0152\n",
      "epoch[5/5], MiniBatch-205 loss:0.0153\n",
      "epoch[5/5], MiniBatch-206 loss:0.0154\n",
      "epoch[5/5], MiniBatch-207 loss:0.0155\n",
      "epoch[5/5], MiniBatch-208 loss:0.0158\n",
      "epoch[5/5], MiniBatch-209 loss:0.0158\n",
      "epoch[5/5], MiniBatch-210 loss:0.0155\n",
      "epoch[5/5], MiniBatch-211 loss:0.0158\n",
      "epoch[5/5], MiniBatch-212 loss:0.0156\n",
      "epoch[5/5], MiniBatch-213 loss:0.0158\n",
      "epoch[5/5], MiniBatch-214 loss:0.0155\n",
      "epoch[5/5], MiniBatch-215 loss:0.0156\n",
      "epoch[5/5], MiniBatch-216 loss:0.0157\n",
      "epoch[5/5], MiniBatch-217 loss:0.0159\n",
      "epoch[5/5], MiniBatch-218 loss:0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[5/5], MiniBatch-219 loss:0.0153\n",
      "epoch[5/5], MiniBatch-220 loss:0.0158\n",
      "epoch[5/5], MiniBatch-221 loss:0.0158\n",
      "epoch[5/5], MiniBatch-222 loss:0.0157\n",
      "epoch[5/5], MiniBatch-223 loss:0.0155\n",
      "epoch[5/5], MiniBatch-224 loss:0.0152\n",
      "epoch[5/5], MiniBatch-225 loss:0.0156\n",
      "epoch[5/5], MiniBatch-226 loss:0.0158\n",
      "epoch[5/5], MiniBatch-227 loss:0.0161\n",
      "epoch[5/5], MiniBatch-228 loss:0.0155\n",
      "epoch[5/5], MiniBatch-229 loss:0.0156\n",
      "epoch[5/5], MiniBatch-230 loss:0.0156\n",
      "epoch[5/5], MiniBatch-231 loss:0.0158\n",
      "epoch[5/5], MiniBatch-232 loss:0.0156\n",
      "epoch[5/5], MiniBatch-233 loss:0.0154\n",
      "epoch[5/5], MiniBatch-234 loss:0.0160\n",
      "epoch[5/5], MiniBatch-235 loss:0.0159\n",
      "epoch[5/5], MiniBatch-236 loss:0.0157\n",
      "epoch[5/5], MiniBatch-237 loss:0.0156\n",
      "epoch[5/5], MiniBatch-238 loss:0.0157\n",
      "epoch[5/5], MiniBatch-239 loss:0.0155\n",
      "epoch[5/5], MiniBatch-240 loss:0.0156\n",
      "epoch[5/5], MiniBatch-241 loss:0.0154\n",
      "epoch[5/5], MiniBatch-242 loss:0.0157\n",
      "epoch[5/5], MiniBatch-243 loss:0.0159\n",
      "epoch[5/5], MiniBatch-244 loss:0.0157\n",
      "epoch[5/5], MiniBatch-245 loss:0.0155\n",
      "epoch[5/5], MiniBatch-246 loss:0.0156\n",
      "epoch[5/5], MiniBatch-247 loss:0.0163\n",
      "epoch[5/5], MiniBatch-248 loss:0.0156\n",
      "epoch[5/5], MiniBatch-249 loss:0.0156\n",
      "epoch[5/5], MiniBatch-250 loss:0.0160\n",
      "epoch[5/5], MiniBatch-251 loss:0.0158\n",
      "epoch[5/5], MiniBatch-252 loss:0.0157\n",
      "epoch[5/5], MiniBatch-253 loss:0.0160\n",
      "epoch[5/5], MiniBatch-254 loss:0.0153\n",
      "epoch[5/5], MiniBatch-255 loss:0.0154\n",
      "epoch[5/5], MiniBatch-256 loss:0.0155\n",
      "epoch[5/5], MiniBatch-257 loss:0.0154\n",
      "epoch[5/5], MiniBatch-258 loss:0.0156\n",
      "epoch[5/5], MiniBatch-259 loss:0.0157\n",
      "epoch[5/5], MiniBatch-260 loss:0.0156\n",
      "epoch[5/5], MiniBatch-261 loss:0.0155\n",
      "epoch[5/5], MiniBatch-262 loss:0.0156\n",
      "epoch[5/5], MiniBatch-263 loss:0.0155\n",
      "epoch[5/5], MiniBatch-264 loss:0.0162\n",
      "epoch[5/5], MiniBatch-265 loss:0.0157\n",
      "epoch[5/5], MiniBatch-266 loss:0.0156\n",
      "epoch[5/5], MiniBatch-267 loss:0.0160\n",
      "epoch[5/5], MiniBatch-268 loss:0.0153\n",
      "epoch[5/5], MiniBatch-269 loss:0.0158\n",
      "epoch[5/5], MiniBatch-270 loss:0.0156\n",
      "epoch[5/5], MiniBatch-271 loss:0.0158\n",
      "epoch[5/5], MiniBatch-272 loss:0.0156\n",
      "epoch[5/5], MiniBatch-273 loss:0.0156\n",
      "epoch[5/5], MiniBatch-274 loss:0.0156\n",
      "epoch[5/5], MiniBatch-275 loss:0.0160\n",
      "epoch[5/5], MiniBatch-276 loss:0.0154\n",
      "epoch[5/5], MiniBatch-277 loss:0.0156\n",
      "epoch[5/5], MiniBatch-278 loss:0.0155\n",
      "epoch[5/5], MiniBatch-279 loss:0.0156\n",
      "epoch[5/5], MiniBatch-280 loss:0.0161\n",
      "epoch[5/5], MiniBatch-281 loss:0.0153\n",
      "epoch[5/5], MiniBatch-282 loss:0.0154\n",
      "epoch[5/5], MiniBatch-283 loss:0.0159\n",
      "epoch[5/5], MiniBatch-284 loss:0.0159\n",
      "epoch[5/5], MiniBatch-285 loss:0.0160\n",
      "epoch[5/5], MiniBatch-286 loss:0.0156\n",
      "epoch[5/5], MiniBatch-287 loss:0.0158\n",
      "epoch[5/5], MiniBatch-288 loss:0.0160\n",
      "epoch[5/5], MiniBatch-289 loss:0.0159\n",
      "epoch[5/5], MiniBatch-290 loss:0.0155\n",
      "epoch[5/5], MiniBatch-291 loss:0.0153\n",
      "epoch[5/5], MiniBatch-292 loss:0.0157\n",
      "epoch[5/5], MiniBatch-293 loss:0.0155\n",
      "epoch[5/5], MiniBatch-294 loss:0.0158\n",
      "epoch[5/5], MiniBatch-295 loss:0.0158\n",
      "epoch[5/5], MiniBatch-296 loss:0.0162\n",
      "epoch[5/5], MiniBatch-297 loss:0.0158\n",
      "epoch[5/5], MiniBatch-298 loss:0.0158\n",
      "epoch[5/5], MiniBatch-299 loss:0.0156\n",
      "epoch[5/5], MiniBatch-300 loss:0.0158\n",
      "epoch[5/5], MiniBatch-301 loss:0.0157\n",
      "epoch[5/5], MiniBatch-302 loss:0.0161\n",
      "epoch[5/5], MiniBatch-303 loss:0.0154\n",
      "epoch[5/5], MiniBatch-304 loss:0.0163\n",
      "epoch[5/5], MiniBatch-305 loss:0.0161\n",
      "epoch[5/5], MiniBatch-306 loss:0.0155\n",
      "epoch[5/5], MiniBatch-307 loss:0.0160\n",
      "epoch[5/5], MiniBatch-308 loss:0.0155\n",
      "epoch[5/5], MiniBatch-309 loss:0.0156\n",
      "epoch[5/5], MiniBatch-310 loss:0.0157\n",
      "epoch[5/5], MiniBatch-311 loss:0.0156\n",
      "epoch[5/5], MiniBatch-312 loss:0.0151\n",
      "epoch[5/5], MiniBatch-313 loss:0.0164\n",
      "epoch[5/5], MiniBatch-314 loss:0.0161\n",
      "epoch[5/5], MiniBatch-315 loss:0.0159\n",
      "epoch[5/5], MiniBatch-316 loss:0.0159\n",
      "epoch[5/5], MiniBatch-317 loss:0.0159\n",
      "epoch[5/5], MiniBatch-318 loss:0.0159\n",
      "epoch[5/5], MiniBatch-319 loss:0.0157\n",
      "epoch[5/5], MiniBatch-320 loss:0.0158\n",
      "epoch[5/5], MiniBatch-321 loss:0.0155\n",
      "epoch[5/5], MiniBatch-322 loss:0.0158\n",
      "epoch[5/5], MiniBatch-323 loss:0.0158\n",
      "epoch[5/5], MiniBatch-324 loss:0.0157\n",
      "epoch[5/5], MiniBatch-325 loss:0.0160\n",
      "epoch[5/5], MiniBatch-326 loss:0.0157\n",
      "epoch[5/5], MiniBatch-327 loss:0.0160\n",
      "epoch[5/5], MiniBatch-328 loss:0.0156\n",
      "epoch[5/5], MiniBatch-329 loss:0.0161\n",
      "epoch[5/5], MiniBatch-330 loss:0.0162\n",
      "epoch[5/5], MiniBatch-331 loss:0.0157\n",
      "epoch[5/5], MiniBatch-332 loss:0.0157\n",
      "epoch[5/5], MiniBatch-333 loss:0.0155\n",
      "epoch[5/5], MiniBatch-334 loss:0.0156\n",
      "epoch[5/5], MiniBatch-335 loss:0.0156\n",
      "epoch[5/5], MiniBatch-336 loss:0.0158\n",
      "epoch[5/5], MiniBatch-337 loss:0.0157\n",
      "epoch[5/5], MiniBatch-338 loss:0.0163\n",
      "epoch[5/5], MiniBatch-339 loss:0.0155\n",
      "epoch[5/5], MiniBatch-340 loss:0.0163\n",
      "epoch[5/5], MiniBatch-341 loss:0.0157\n",
      "epoch[5/5], MiniBatch-342 loss:0.0156\n",
      "epoch[5/5], MiniBatch-343 loss:0.0159\n",
      "epoch[5/5], MiniBatch-344 loss:0.0164\n",
      "epoch[5/5], MiniBatch-345 loss:0.0161\n",
      "epoch[5/5], MiniBatch-346 loss:0.0158\n",
      "epoch[5/5], MiniBatch-347 loss:0.0163\n",
      "epoch[5/5], MiniBatch-348 loss:0.0159\n",
      "epoch[5/5], MiniBatch-349 loss:0.0156\n",
      "epoch[5/5], MiniBatch-350 loss:0.0161\n",
      "epoch[5/5], MiniBatch-351 loss:0.0157\n",
      "epoch[5/5], MiniBatch-352 loss:0.0164\n",
      "epoch[5/5], MiniBatch-353 loss:0.0154\n",
      "epoch[5/5], MiniBatch-354 loss:0.0156\n",
      "epoch[5/5], MiniBatch-355 loss:0.0156\n",
      "epoch[5/5], MiniBatch-356 loss:0.0159\n",
      "epoch[5/5], MiniBatch-357 loss:0.0161\n",
      "epoch[5/5], MiniBatch-358 loss:0.0157\n",
      "epoch[5/5], MiniBatch-359 loss:0.0159\n",
      "epoch[5/5], MiniBatch-360 loss:0.0158\n",
      "epoch[5/5], MiniBatch-361 loss:0.0159\n",
      "epoch[5/5], MiniBatch-362 loss:0.0160\n",
      "epoch[5/5], MiniBatch-363 loss:0.0160\n",
      "epoch[5/5], MiniBatch-364 loss:0.0164\n",
      "epoch[5/5], MiniBatch-365 loss:0.0160\n",
      "epoch[5/5], MiniBatch-366 loss:0.0157\n",
      "epoch[5/5], MiniBatch-367 loss:0.0158\n",
      "epoch[5/5], MiniBatch-368 loss:0.0158\n",
      "epoch[5/5], MiniBatch-369 loss:0.0160\n",
      "epoch[5/5], MiniBatch-370 loss:0.0160\n",
      "epoch[5/5], MiniBatch-371 loss:0.0158\n",
      "epoch[5/5], MiniBatch-372 loss:0.0155\n",
      "epoch[5/5], MiniBatch-373 loss:0.0161\n",
      "epoch[5/5], MiniBatch-374 loss:0.0160\n",
      "epoch[5/5], MiniBatch-375 loss:0.0156\n",
      "epoch[5/5], MiniBatch-376 loss:0.0161\n",
      "epoch[5/5], MiniBatch-377 loss:0.0156\n",
      "epoch[5/5], MiniBatch-378 loss:0.0159\n",
      "epoch[5/5], MiniBatch-379 loss:0.0159\n",
      "epoch[5/5], MiniBatch-380 loss:0.0158\n",
      "epoch[5/5], MiniBatch-381 loss:0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:42<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[5/5], [-----TEST------] loss:0.0349  Accur:0.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = float(\"inf\")\n",
    "best_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #-------- Training -------------------\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    running_loss = []\n",
    "    if epoch >= teach_force_till: teacher_forcing = 0\n",
    "    else: teacher_forcing = max(0, teacher_forcing - teach_decay_pereph)\n",
    "\n",
    "    for ith, (src, tgt, src_sz) in enumerate(train_dataloader):\n",
    "\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        #--- forward ------\n",
    "        output = model(src = src, tgt = tgt, src_sz =src_sz,\n",
    "                       teacher_forcing_ratio = teacher_forcing)\n",
    "        loss = loss_estimator(output, tgt) / acc_grad\n",
    "        acc_loss += loss\n",
    "\n",
    "        #--- backward ------\n",
    "        loss.backward()\n",
    "        if ( (ith+1) % acc_grad == 0):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print('epoch[{}/{}], MiniBatch-{} loss:{:.4f}'\n",
    "                .format(epoch+1, num_epochs, (ith+1)//acc_grad, acc_loss.data))\n",
    "            running_loss.append(acc_loss.item())\n",
    "            acc_loss=0\n",
    "            # break\n",
    "\n",
    "    LOG2CSV(running_loss, LOG_PATH+\"trainLoss.csv\")\n",
    "\n",
    "    #--------- Validate ---------------------\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    for jth, (v_src, v_tgt, v_src_sz) in enumerate(tqdm(train_dataloader)):\n",
    "        v_src = v_src.to(device)\n",
    "        v_tgt = v_tgt.to(device)\n",
    "        with torch.no_grad():\n",
    "            v_output = model(src = v_src, tgt = v_tgt, src_sz = v_src_sz)\n",
    "            val_loss += loss_estimator(v_output, v_tgt)\n",
    "\n",
    "            val_accuracy += accuracy_score(v_output, v_tgt, tgt_glyph) # in Utils section\n",
    "        # break\n",
    "    val_loss = val_loss / len(train_dataloader)\n",
    "    val_accuracy = val_accuracy / len(train_dataloader)\n",
    "\n",
    "    print('epoch[{}/{}], [-----TEST------] loss:{:.4f}  Accur:{:.4f}'\n",
    "            .format(epoch+1, num_epochs, val_loss.data, val_accuracy.data))\n",
    "    LOG2CSV([val_loss.item(), val_accuracy.item()],\n",
    "                LOG_PATH+\"valLoss.csv\")\n",
    "\n",
    "    #-------- save Checkpoint -------------------\n",
    "    if val_accuracy > best_accuracy:\n",
    "    # if val_loss < best_loss:\n",
    "        print(\"***saving best optimal state [Loss:{} Accur:{}] ***\".format(val_loss.data,val_accuracy.data) )\n",
    "        best_loss = val_loss\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), WGT_PREFIX+\"_model.pth\")\n",
    "        LOG2CSV([epoch+1, val_loss.item(), val_accuracy.item()],\n",
    "                LOG_PATH+\"bestCheckpoint.csv\")\n",
    "\n",
    "    # LR step\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCfDiXgyPUoJ"
   },
   "source": [
    "# Inference & Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdO4DI8aXh90"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtLtiPSvV18V"
   },
   "source": [
    "###JSON handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "uGUTK0AwVrGf"
   },
   "outputs": [],
   "source": [
    "def save_to_json(path, data_dict):\n",
    "    with open(path ,\"w\", encoding = \"utf-8\") as f:\n",
    "        json.dump(data_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
    "\n",
    "\n",
    "def toggle_json(read_path, save_prefix=\"\"):\n",
    "    with open(read_path, 'r', encoding = \"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    tog_dict = dict()\n",
    "    for d in data.keys():\n",
    "        for v in data[d]:\n",
    "            tog_dict[v] = set()\n",
    "\n",
    "    for d in data.keys():\n",
    "        for v in data[d]:\n",
    "            tog_dict[v].add(d)\n",
    "\n",
    "    for t in tog_dict.keys():\n",
    "        tog_dict[t] = list(tog_dict[t])\n",
    "\n",
    "    save_file = save_prefix+\"/Toggled-\"+ os.path.basename(read_path)\n",
    "    with open(save_file,\"w\", encoding = \"utf-8\") as f:\n",
    "        json.dump(tog_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
    "\n",
    "    return save_file\n",
    "\n",
    "\n",
    "def get_from_json(path, ret_data = \"key\"):\n",
    "    with open(path, 'r', encoding = \"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if ret_data == \"key\":\n",
    "        out = list(data.keys())\n",
    "    elif ret_data == \"value\":\n",
    "        temp = data.values()\n",
    "        temp = { i for t in temp for i in t }\n",
    "        out = list(temp)\n",
    "    elif ret_data == \"both\":\n",
    "        out = []\n",
    "        for k in data.keys():\n",
    "            for v in data[k]:\n",
    "                out.append([k,v])\n",
    "    return sorted(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO-MBJ6rU6EV"
   },
   "source": [
    "###Inference Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR7kl3yJhhdX"
   },
   "source": [
    "Reranking routine based on monolingual vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "Fr36S5Cfhd9Y"
   },
   "outputs": [],
   "source": [
    "class VocabSanitizer():\n",
    "    '''\n",
    "    Sanitize topK vocab prediction using ancillary vocab list\n",
    "    by reranking or removing etc\n",
    "    '''\n",
    "    def __init__(self, data_file):\n",
    "        '''\n",
    "        data_file: path to file conatining vocabulary list\n",
    "        '''\n",
    "        extension = os.path.splitext(data_file)[-1]\n",
    "        if extension == \".json\":\n",
    "            self.vocab_set  = set( json.load(open(data_file)) )\n",
    "        elif extension == \".csv\":\n",
    "            self.vocab_df = pd.read_csv(data_file).set_index('WORD')\n",
    "            self.vocab_set = set( self.vocab_df.index )\n",
    "        else:\n",
    "            print(\"Only Json/CSV file extension supported\")\n",
    "\n",
    "\n",
    "    def remove_astray(self, word_list):\n",
    "        '''Remove words that are not present in vocabulary\n",
    "        '''\n",
    "        new_list = []\n",
    "        for v in word_list:\n",
    "            if v in self.vocab_set:\n",
    "                new_list.append(v)\n",
    "        if new_list == []:\n",
    "            return word_list.copy()\n",
    "            # return [\" \"]\n",
    "        return new_list\n",
    "\n",
    "    def reposition(self, word_list):\n",
    "        '''Reorder Words in list\n",
    "        '''\n",
    "        new_list = []\n",
    "        temp_ = word_list.copy()\n",
    "        for v in word_list:\n",
    "            if v in self.vocab_set:\n",
    "                new_list.append(v)\n",
    "                temp_.remove(v)\n",
    "        new_list.extend(temp_)\n",
    "\n",
    "        return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwoM0N9rhe_c"
   },
   "source": [
    "Inference runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "PcexjrRlR8Rf"
   },
   "outputs": [],
   "source": [
    "def inferencer(word, topk = 10):\n",
    "    in_vec = torch.from_numpy(src_glyph.word2xlitvec(word)).to(device)\n",
    "    ## change to active or passive beam\n",
    "    p_out_list = model.active_beam_inference(in_vec, beam_width = topk)\n",
    "    p_result = [ tgt_glyph.xlitvec2word(out.cpu().numpy()) for out in p_out_list]\n",
    "\n",
    "    result = p_result\n",
    "    # result = voc_sanitize.reposition(p_result) ## Uncomment for repositioning\n",
    "\n",
    "    return result\n",
    "\n",
    "def inference_looper(in_words, topk = 3):\n",
    "    out_dict = {}\n",
    "    for i in tqdm(in_words):\n",
    "        out_dict[i] = inferencer(i, topk=topk)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNFe5RpLVCNj"
   },
   "source": [
    "##Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "QjMkNDjDLCOo"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "WGT_PATH = INST_NAME+\"/weights/\"+INST_NAME+\"_model.pth\"\n",
    "\n",
    "SAVE_DIR = LOG_PATH + \"/acc_log/\"\n",
    "if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)\n",
    "\n",
    "# voc_sanitize = VocabSanitizer(\"checkup_words_sorted.json\") #Monolingual based topK sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5YFsfB3bLtn",
    "outputId": "fd73678f-38f5-4ccd-86aa-d6d5883acfd2"
   },
   "outputs": [],
   "source": [
    "#Loading Accuracy Computing script\n",
    "!wget https://raw.githubusercontent.com/AI4Bharat/IndianNLP-Transliteration/jgeob-dev/tools/accuracy_reporter/accuracy_news.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MH09odiVHLc",
    "outputId": "530ab40d-a538-427c-9566-56fc68864971"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [19:26<00:00,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.json\n",
      "\n",
      "TOP 1 SCORES FOR 194 SAMPLES:\n",
      "ACC:          0.953608\n",
      "Mean F-score: 0.991101\n",
      "MRR:          0.953608\n",
      "MAP_ref:      0.953608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tfi =  toggle_json(TEST_FILE, save_prefix=SAVE_DIR)\n",
    "words = get_from_json(tfi, \"key\")\n",
    "model.to(device)\n",
    "out_dict = inference_looper(words, topk = 1)\n",
    "\n",
    "pred_path = os.path.join(SAVE_DIR, \"pred_\"+os.path.basename(TEST_FILE) )\n",
    "save_to_json(pred_path, out_dict)\n",
    "\n",
    "gt_json = tfi\n",
    "pred_json = pred_path\n",
    "save_prefix = os.path.join(SAVE_DIR, os.path.basename(TEST_FILE).replace(\".json\", \"\"))\n",
    "\n",
    "for topk in [1]:\n",
    "    ## GT json file passed to below script must be in { En(input): [NativeLang (predict)] } format\n",
    "    run_accuracy_news = \"( echo {} && python accuracy_news.py --gt-json {} --pred-json {} --topk {} --save-output-csv {}_top{}-scores.csv ) | tee -a {}/Summary.txt\".format(\n",
    "                    os.path.basename(TRAIN_FILE),\n",
    "                    gt_json, pred_json, topk,\n",
    "                    save_prefix, topk, SAVE_DIR )\n",
    "\n",
    "    os.system(run_accuracy_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9t58n8tp0TwO",
    "outputId": "2b6e3396-66b4-4130-ceb2-233ad8d76875"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'بويكوت': ['boykot']}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "tfi =  toggle_json(TEST_FILE, save_prefix='')\n",
    "words = get_from_json(tfi, \"key\")[:10]\n",
    "out_dict = inference_looper(['بويكوت'], topk = 1)\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3loJmEdTkaMH"
   },
   "source": [
    "#Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tUBGDF-gTCw",
    "outputId": "f6a55599-9af6-4fbb-8443-e15869d9f8a5"
   },
   "outputs": [],
   "source": [
    "# Compress Logs anad Model for Download\n",
    "!zip -r j2r_10_epoch.zip {INST_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"jawi_j2r_glyph.pkl\", \"wb\") as file:\n",
    "    pickle.dump(src, file)\n",
    "    \n",
    "with open(\"rumi_j2r_glyph.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tgt_glyph, file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-jfVIZzEKHZB",
    "WjNms0lEji1_",
    "fdy2XH9fldK8",
    "2LC0tg-klP9D",
    "-XLm3rKEmVma",
    "0pTRIOhkJ1pY",
    "k81RNgwckP3w",
    "s54j8UNIOHuS",
    "SdO4DI8aXh90"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
